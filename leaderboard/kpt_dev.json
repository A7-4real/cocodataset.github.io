[{"description": "A single network for multi-person keypoint localization based on the stacked hourglass design (Newell et al. ECCV 2016). No additional training data or separate person detection is used. For each body joint at each pixel location, our network predicts a detection score, plus a scalar score that serves as a \u201ctag\u201d. Body joints with similar tags are grouped together to form individual persons. The detection scores and tags are jointly trained.", "leaderboard_name": "kpt-dev", "url": "https://github.com/anewell/pose-hg-train", "results": "{\"AP_50\": 0.746, \"AR_medium\": 0.407, \"AR_50\": 0.771, \"AP_large\": 0.556, \"AR_75\": 0.546, \"AP\": 0.46, \"AP_medium\": 0.388, \"AR\": 0.518, \"AR_large\": 0.669, \"AP_75\": 0.484}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1235, "members": "Alejandro Newell, Jia Deng", "name": "umich_vl"}, "date": "2016-09-16", "publication": "https://arxiv.org/abs/1603.06937"}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.295, \"AR_medium\": 0.105, \"AR_50\": 0.43, \"AP_large\": 0.137, \"AR_75\": 0.142, \"AP\": 0.103, \"AP_medium\": 0.081, \"AR\": 0.188, \"AR_large\": 0.3, \"AP_75\": 0.054}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1236, "members": "", "name": "zjhuang"}, "date": "2016-09-16", "publication": ""}, {"description": "We present a bottom-up method that first detects individual human joints and then connects joints to form bodies using a limb affinity score. Both the joint detections and limb affinity scores are predicted jointly by a fully convolutional neural network, based on the iterated prediction architecture of convolutional pose machines. A greedy algorithm then connects limbs into subsets to form individual person detections. The system was trained only on MSCOCO data.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.849, \"AR_medium\": 0.606, \"AR_50\": 0.872, \"AP_large\": 0.682, \"AR_75\": 0.718, \"AP\": 0.618, \"AP_medium\": 0.571, \"AR\": 0.665, \"AR_large\": 0.746, \"AP_75\": 0.675}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1237, "members": "Zhe Cao, Shih-En Wei, Tomas Simon, Yaser Sheikh", "name": "CMU-Pose"}, "date": "2016-09-16", "publication": ""}, {"description": "The G-RMI detection entry is a two-stage system consisting of a bounding box person detector followed by a human pose estimator. For person bounding box detection we use a combination of G-RMI's generic ResNet-Inception-based object detector trained only on MS-COCO data, as well as a person-specific box detector trained on both MS-COCO and in-house data. For pose estimation we use a single ResNet-101 based model trained to predict dense keypoint heatmap and offset fields, whose fusion yields highly localized keypoint activation maps which are used for both keypoint localization and proposal rescoring. The pose estimator has been trained on both MS-COCO and in-house data.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.822, \"AR_medium\": 0.619, \"AR_50\": 0.866, \"AP_large\": 0.666, \"AR_75\": 0.714, \"AP\": 0.605, \"AP_medium\": 0.576, \"AR\": 0.662, \"AR_large\": 0.722, \"AP_75\": 0.662}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1238, "members": "Nori Kanazawa*, George Papandreou*, Alex Toshev*, Tyler Zhu*, Hartwig Adam, Chris Bregler, Kevin Murphy, Jonathan Tompson, (alphabetical order, core system developers marked with asterisk)", "name": "G-RMI"}, "date": "2016-09-16", "publication": ""}, {"description": "We have implemented a \u00ab specialist model \u00bb that specializes its last layes on specific body parts. To do so we have stacked in parrallel, block of convolutional layers on features map of a pertained model (VGG16), and we have jointly trained these regressor on the task of keypoint detection.", "leaderboard_name": "kpt-dev", "url": "https://github.com/NaasCraft/SSR_COCO", "results": "{\"AP_50\": 0.084, \"AR_medium\": 0.069, \"AR_50\": 0.256, \"AP_large\": 0.02, \"AR_75\": 0.022, \"AP\": 0.017, \"AP_medium\": 0.018, \"AR\": 0.072, \"AR_large\": 0.077, \"AP_75\": 0.001}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1239, "members": "Raphael Canyasse, Guillaume Demonet, Charles Sutton", "name": "USSR"}, "date": "2016-09-16", "publication": ""}, {"description": "1. Use faster RCNN to generate a lot of bounding boxes.  2. Use single-person pose estimator to generate keypoints regression.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.75, \"AR_medium\": 0.499, \"AR_50\": 0.77, \"AP_large\": 0.567, \"AR_75\": 0.61, \"AP\": 0.514, \"AP_medium\": 0.474, \"AR\": 0.563, \"AR_large\": 0.649, \"AP_75\": 0.559}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1240, "members": "Yuxiang Peng, Gang Yu", "name": "R4D"}, "date": "2016-09-16", "publication": ""}, {"description": "The main idea of our method is we first samples a small number of representable pixels and then determine their labels via a convolutional layer followed by a softmax layer. We develop a CNN architecture to predict the key points from different layers, then fuse all the side predictions as the final prediction.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.753, \"AR_medium\": 0.753, \"AR_50\": 0.827, \"AP_large\": 0.543, \"AR_75\": 0.692, \"AP\": 0.544, \"AP_medium\": 0.583, \"AR\": 0.708, \"AR_large\": 0.768, \"AP_75\": 0.509}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1241, "members": "Shaoli Huang, Dacheng Tao", "name": "DL-61"}, "date": "2016-09-16", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.617, \"AR_medium\": 0.316, \"AR_50\": 0.63, \"AP_large\": 0.58, \"AR_75\": 0.489, \"AP\": 0.419, \"AP_medium\": 0.3, \"AR\": 0.454, \"AR_large\": 0.639, \"AP_75\": 0.452}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1242, "members": "", "name": "belagian"}, "date": "2016-09-16", "publication": ""}, {"description": "A pipeline approach with Multibox [1] to detect people and Stacked Hourglass Networks [2] to estimate their pose. [1] Scalable High Quality Object Detection. Erhan et al., CVPR 2014 [2] Stacked Hourglass Networks for Human Pose Estimation. Newell et al., POCV 2016", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.652, \"AR_medium\": 0.448, \"AR_50\": 0.8, \"AP_large\": 0.492, \"AR_75\": 0.566, \"AP\": 0.402, \"AP_medium\": 0.349, \"AR\": 0.538, \"AR_large\": 0.659, \"AP_75\": 0.419}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1243, "members": "Grant Van Horn, Matteo Ruggero Ronchi, David Hall", "name": "Caltech"}, "date": "2016-09-16", "publication": ""}, {"description": "Our keypoint results were obtained by single-person skeleton models, on the human bounding boxes generated by MegDet, a large-batch object detector by Megvii (Face++). The single-person skeleton model is based on a ResNet with U-shape side branch to integrate high resolution information from the earlier layers and semantic information from the deeper layers. Large kernel [1] convolution was utilized to enlarge the effective receptive field. Ensemble was built on the feature-map level to generate the final results. Our method was trained on the provided labeled images only.", "leaderboard_name": "kpt-dev", "url": "https://arxiv.org/abs/1711.07319", "results": "{\"AP_50\": 0.917, \"AR_medium\": 0.748, \"AR_50\": 0.951, \"AP_large\": 0.781, \"AR_75\": 0.859, \"AP\": 0.730, \"AP_medium\": 0.695, \"AR\": 0.790, \"AR_large\": 0.846, \"AP_75\": 0.809}", "results_details": {"ensemble": true, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1244, "members": "Yilun Chen*, Zhicheng Wang*, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Jian Sun (* indicates equal contribution); Megvii Research", "name": "Megvii (Face++)"}, "date": "2017-10-29", "publication": "https://arxiv.org/abs/1703.02719"}, {"description": "We implement a two stage top-down method. In the first stage, we use mask rcnn like framework to detect human bounding boxes under both supervision from detection and keypoints. In the second stage, we perform single person pose estimation. In addition, we remove false annotated data based on the keypoint annotation and detection annotation jointly. We also use both detection and keypoint score for instance re-scoring and perform box-nms and oks-nms based on it. In both stages we use only single model, no model ensembles are applied. Additional data are from a public dataset of AI-Challenger keypoints detection competition track and provide 1.5 point AP improvement on the validation dataset.", "leaderboard_name": "kpt-dev", "url": "https://github.com/MVIG-SJTU/RMPE", "results": "{\"AP_50\": 0.903, \"AR_medium\": 0.725, \"AR_50\": 0.939, \"AP_large\": 0.784, \"AR_75\": 0.840, \"AP\": 0.720, \"AP_medium\": 0.676, \"AR\": 0.771, \"AR_large\": 0.835, \"AP_75\": 0.797}", "results_details": {"ensemble": false, "external_data": true}, "leaderboard_id": "12", "team": {"id": 1245, "members": "Changbao Wang (Beihang University), Yujie Wang (Beihang University), Quanquan Li (SenseTime Group Limited)", "name": "oks"}, "date": "2017-10-29", "publication": ""}, {"description": "We build a two-stage system consisting of a bounding-box person detector followed by a human pose estimator. For person detection, we train a faster rcnn with deformable version of resnet101 on MS-COCO trainval2017. Multi-scale testing and soft-NMS are used. For pose estimation, we train a resnet-101 based model to predict heatmap and offset fields of keypoints for the person in the detected bounding box. The performance is boosted by OKS-based NMS in post-processing. The pose estimator is trained using COCO MS-trainval2017 and extra dataset. For single model, we obtain an average precision of 0.713 on the test-dev set and 0.688 on the test-challenge set. Finally, we use an ensemble of seven models and obtain an average precision of 0.728 on the test-dev set and 0.706 on the test-challenge set.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.894, \"AR_medium\": 0.736, \"AR_50\": 0.941, \"AP_large\": 0.800, \"AR_75\": 0.848, \"AP\": 0.728, \"AP_medium\": 0.686, \"AR\": 0.787, \"AR_large\": 0.856, \"AP_75\": 0.796}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": "12", "team": {"id": 1246, "members": "Xinze Chen*, Rui Wu*, Yun Ye and Guan Huang (*The first two authors contribute equally to this work)", "name": "bangbangren"}, "date": "2017-10-29", "publication": ""}, {"description": "Our method is based on the two-stage approach described in our CVPR 2017 paper (https://arxiv.org/abs/1701.01779), with some improvements (use of a Resnet-152 instead of a ResNet-101 backbone, better feature alignment, more refined training schedule, etc). Our final entry in the competition uses a single model (no ensembling). In addition to the COCO images and annotations (and Imagenet image classification pretraining), we also used for training our model an internal dataset of images with keypoint annotations which is described in detail in Sec. 4.1 of our paper (same number of extra annotations as our last year's entry to the competition). These extra data improved keypoints test-dev AP by 3-4%.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.879, \"AR_medium\": 0.714, \"AR_50\": 0.912, \"AP_large\": 0.752, \"AR_75\": 0.819, \"AP\": 0.710, \"AP_medium\": 0.690, \"AR\": 0.758, \"AR_large\": 0.820, \"AP_75\": 0.777}", "results_details": {"ensemble": false, "external_data": true}, "leaderboard_id": "12", "team": {"id": 1238, "members": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Hartwig Adam, Kevin Murphy (Google Research)", "name": "G-RMI"}, "date": "2017-10-29", "publication": "https://arxiv.org/abs/1701.01779"}, {"description": "Our keypoint entry is build upon the several papers developed at FAIR: (i) Mask R-CNN [1], for bbox and keypoint prediction; (ii) Feature Pyramid Networks (FPN) [2], and (iii) ResNeXt [3] 101-32x8d pre-trained on ImageNet-5k as in [3], as the backbone. In this entry, we highlight our newly developed large-scale semi-supervised learning paradigm which we call Data Distillation [4, to appear]. We apply Mask R-CNN with test-time augmentation on a large set of unlabeled images (Sports-1M frames without using any annotations), and we view these predictions as new annotations which we combine with the COCO images to retrain our model. No data labeled with human boxes, masks, or keypoints, beyond what is provided in COCO, are used in our entry. Main results on test-dev include: (i) Single-model, Mask R-CNN baseline, with FPN and ResNeXt-101-32x8d: 65.8 keypoint AP (ii) Single-model, plus Data Distillation from unlabeled Sports-1M: 67.0 keypoint AP  (iii) Single-model, Data Distillation plus test time augmentations (horizontal flip, multi-scale): 69.2 keypoint AP [1] https://arxiv.org/abs/1703.06870 [2] https://arxiv.org/abs/1612.03144 [3] https://arxiv.org/abs/1611.05431 [4] in preparation.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.904, \"AR_medium\": 0.703, \"AR_50\": 0.937, \"AP_large\": 0.763, \"AR_75\": 0.811, \"AP\": 0.692, \"AP_medium\": 0.649, \"AR\": 0.752, \"AR_large\": 0.818, \"AP_75\": 0.760}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1248, "members": "*Georgia Gkioxari, *Ilija Radosavovic, Kaiming He, Ross Girshick, Piotr Doll\u00e1r (* denotes equal co-first authorship); Facebook AI Research", "name": "FAIR Mask R-CNN"}, "date": "2017-10-29", "publication": "https://arxiv.org/abs/1703.06870"}, {"description": "We propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections. We train our single person pose estimator on both COCO and AI challenger dataset. Our single model performance achieved 68.5 mAP on test-dev set. We were able to achieve 69.7 mAP on test-dev set by fusing 3 models after the competition. The inference time for an image is around 1-2s.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.875, \"AR_medium\": 0.689, \"AR_50\": 0.910, \"AP_large\": 0.751, \"AR_75\": 0.798, \"AP\": 0.688, \"AP_medium\": 0.646, \"AR\": 0.736, \"AR_large\": 0.802, \"AP_75\": 0.759}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": "12", "team": {"id": 1249, "members": "Hao-Shu Fang, Shanghai Jiao Tong University; Jiefeng Li, Shanghai Jiao Tong University; Ruiheng Chang, Shanghai Jiao Tong University; Jinkun Cao, Shanghai Jiao Tong University; Yinghong Fang, Shanghai Jiao Tong University; Yu-Wing Tai, Tencent YouTu; Cewu Lu(*corresponding author), Shanghai Jiao Tong University", "name": "SJTU"}, "date": "2017-10-29", "publication": ""}, {"description": "The submitted results are obtained by using the 'divide and combine' technique and AE method [1]. [1]Associative Embedding: End-to-End Learning for Joint Detection and Grouping Newell et al. NIPS 2017.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.852, \"AR_medium\": 0.626, \"AR_50\": 0.885, \"AP_large\": 0.713, \"AR_75\": 0.741, \"AP\": 0.636, \"AP_medium\": 0.582, \"AR\": 0.688, \"AR_large\": 0.774, \"AP_75\": 0.698}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1250, "members": "Yue Liao, Yao Sun, Si Liu, Yinglu Liu, Yanli Li, Junjun Xiong.", "name": "iie-samsung-pose"}, "date": "2017-10-29", "publication": ""}, {"description": "METU is a pipeline approach that combines a person detector (PD) with a single person pose estimator. The erroneous detection from PD was corrected via adaptive thresholding of confidence results.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.678, \"AR_medium\": 0.405, \"AR_50\": 0.720, \"AP_large\": 0.507, \"AR_75\": 0.548, \"AP\": 0.412, \"AP_medium\": 0.355, \"AR\": 0.484, \"AR_large\": 0.591, \"AP_75\": 0.452}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1251, "members": "M. Salih Karagoz, Muhammed Kocabas.", "name": "METU"}, "date": "2017-10-29", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.720, \"AR_medium\": 0.473, \"AR_50\": 0.781, \"AP_large\": 0.548, \"AR_75\": 0.606, \"AP\": 0.451, \"AP_medium\": 0.383, \"AR\": 0.560, \"AR_large\": 0.679, \"AP_75\": 0.476}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1252, "members": "", "name": "Jessie33321"}, "date": "2017-10-29", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.667, \"AR_medium\": 0.566, \"AR_50\": 0.833, \"AP_large\": 0.354, \"AR_75\": 0.663, \"AP\": 0.411, \"AP_medium\": 0.519, \"AR\": 0.620, \"AR_large\": 0.694, \"AP_75\": 0.424}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1253, "members": "", "name": "Gnxr9"}, "date": "2017-10-29", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.550, \"AR_medium\": 0.416, \"AR_50\": 0.767, \"AP_large\": 0.295, \"AR_75\": 0.548, \"AP\": 0.301, \"AP_medium\": 0.366, \"AR\": 0.524, \"AR_large\": 0.670, \"AP_75\": 0.285}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": "12", "team": {"id": 1254, "members": "", "name": "LwhL"}, "date": "2017-10-29", "publication": ""}, {"description": "We take the top-down pipeline by first detecting the person by our MegDet [1] system. The performance of person detection is 55.4 on COCO validation set (mmAP on 80 classes).   We adopt an iterative approach to train the pose estimator. In the first stage, all the data are used for the training. In the latter stage, we sample more difficult data (with low OKS) to refine the estimator. We iterate the training for four times.   Using a ResNet50 as the backbone, our single model can achieve 77.1 on test-dev. An ensemble of four model (using res50 backbone) achieved 78.1 on test-dev and 76.4 on test-challenge. External data was used in our final submission with 0.7 mAP gain on the validation set.  Each model was trained on a single machine with 8 GTX 1080Ti GPU cards.  [1] MegDet: A Large Mini-Batch Object Detector, Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, Jian Sun, CVPR 2018", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.941\", \"AP\": \"0.781\", \"AP_medium\": \"0.745\", \"AR\": \"0.831\", \"AR_50\": \"0.967\", \"AP_large\": \"0.833\", \"AR_large\": \"0.882\", \"AR_75\": \"0.898\", \"AR_medium\": \"0.793\", \"AP_75\": \"0.859\"}", "leaderboard_name": "kpt-dev", "team": {"name": "Megvii (Face++)", "members": "Zhicheng Wang, Wenbo Li,  Binyi Yin, Qixiang Peng, Tianzi Xiao, Yuming Du, Zeming Li, Xiangyu Zhang, Gang Yu,  Jian Sun; Megvii (Face++)", "id": 1244}, "date": "2018-09-09", "publication": ""}, {"description": "Our method is based on our ECCV2018 paper(https://arxiv.org/abs/1804.06208) and our triangle-net which is a multi-branch network for human pose estimation. The final results are got by using extra data(https://challenger.ai/competition/keypoint/subject) for training and models ensemble for test(5 models based on our approach of our ECCV2018 paper and 1 model based our new triangle-net for pose estimation). Our best single model has 0.754 of mAP on COCO test-dev dataset, and ensemble model has 0.765 of mAP on COCO test-dev dataset and 0.745 of mAP on COCO test-std dataset.", "leaderboard_id": "12", "url": "https://arxiv.org/abs/1804.06208", "results": "{\"AP_50\": \"0.924\", \"AP\": \"0.765\", \"AP_medium\": \"0.730\", \"AR\": \"0.815\", \"AR_50\": \"0.958\", \"AP_large\": \"0.827\", \"AR_large\": \"0.872\", \"AR_75\": \"0.882\", \"AR_medium\": \"0.774\", \"AP_75\": \"0.840\"}", "leaderboard_name": "kpt-dev", "team": {"name": "MSRA", "members": "Bin Xiao(Microsoft Research Aisa), Dianqi Li(Microsoft Research), Ke Sun(Microsoft Research Aisa), Lei Zhang(Microsoft Research), Jingdong Wang(Microsoft Research Asia)", "id": 1255}, "date": "2018-09-09", "publication": ""}, {"description": "We built a two-stage system,  which estimated single person skeleton after human boxes were detetected.  We first trained a mask-rcnn similar model with only coco-2017 training set for human detection. Then we trained pose   estimator(s) to predict keypoint heatmap and offset based on detected human box.  Resnet-101, resnet-152 and resnext-152 were used as backbones and Deconv structure was utilized to enlarge final resolution. Coco-2017 trainval set and extra in-house dataset were used in training.  Without any test strategy,  our single pose-estimator got 74.2 AP in test-  dev  and  72.3 AP in test-challeng set.  The final ensemble result used four pose-estimators  with fliping and multi-scale testing strategies, and reached 75.9 AP in test-dev and 74.1 AP in test-challenge set.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.921\", \"AP\": \"0.759\", \"AP_medium\": \"0.717\", \"AR\": \"0.804\", \"AR_50\": \"0.951\", \"AP_large\": \"0.821\", \"AR_large\": \"0.867\", \"AR_75\": \"0.867\", \"AR_medium\": \"0.758\", \"AP_75\": \"0.830\"}", "leaderboard_name": "kpt-dev", "team": {"name": "The Sea Monsters", "members": "Rui Zhang*, Yinglun Liu*, Rui Wu, Xinze Chen, Guan Huang (* indicates equal contribution) ", "id": 1256}, "date": "2018-09-09", "publication": ""}, {"description": "We implement a two stage top-down method. In the first stage, we use mask rcnn to detect human bounding boxes and the person detection AP is about 53 on the test dev set. In the second stage, we perform single person pose estimation. We train Resnet based models to predict heatmap of keypoints for the person in the bounding box. We use Bi-FPN to get multi scale feature map and structure loss to learn the human skeletal structures. Before predicting, we also add a convolutional block attention module to refine the final feature map. During training, we use additional data from AI-Challenger Human Skeletal System Keypoints Detection (2017) and it provides about 1.0 point AP improvement on the validation dataset. For single model, we obtain an AP of 75.2 on the test-dev set. Finally, we use an ensemble of 8 models to obtain an AP of 76.0 on the test-dev set.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.916\", \"AP\": \"0.749\", \"AP_medium\": \"0.710\", \"AR\": \"0.806\", \"AR_50\": \"0.952\", \"AP_large\": \"0.808\", \"AR_large\": \"0.872\", \"AR_75\": \"0.868\", \"AR_medium\": \"0.758\", \"AP_75\": \"0.820\"}", "leaderboard_name": "kpt-dev", "team": {"name": "DGDBQ", "members": "Kunlin Yang (Tsinghua University) Maoqing Tian (Beihang University) Mingyuan Zhang (Beihang University)", "id": 1257}, "date": "2018-09-09", "publication": ""}, {"description": "We use a two-stage method: 1) Faster RCNN + SoftNMS (mAP 47% on COCO test-dev) to detect human boxes; 2) Cascade Pyramid Network with Deconvolution Head to detect human keypoints. We first train models using COCO dataset combined with AI-Challenger dataset, and then finetune on COCO dataset, which brings 1% improvement compared with training on COCO dataset only. We train different versions based on different backbones including: ResNet101 (single model AP: 75% on COCO val, 73.4% on COCO test-dev), Se-ResNeXt101 (single model AP: 74.6% on COCO val), ResNet101-dilation(Large-feature, single model AP: 74.9% on COCO val). Finally we ensemble 4 models including: 1 ResNet101 model + 2 Se-ResNeXt101 models + 1 ResNet101-dilation model and obtain an AP of 75.1% on COCO test-dev and 72.8% on COCO test-challenge.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.918\", \"AP\": \"0.751\", \"AP_medium\": \"0.715\", \"AR\": \"0.809\", \"AR_50\": \"0.954\", \"AP_large\": \"0.812\", \"AR_large\": \"0.869\", \"AR_75\": \"0.871\", \"AR_medium\": \"0.766\", \"AP_75\": \"0.824\"}", "leaderboard_name": "kpt-dev", "team": {"name": "KPLab", "members": "Boyuan Sun, Hang Zhang, Zixuan Guan, Hao Zhao, Haiyong Sun, Zhaonan Wang; (Yi+ AILab from Beijing Moshanghua Tech)", "id": 1258}, "date": "2018-09-09", "publication": ""}, {"description": "We handle the multi-person pose estimation with the top-down pipeline. The channel shuffle module is proposed to promote the cross-channel information communications among the feature maps across all scales, and a spatial, channel-wise attention residual bottleneck is designed to adaptively highlight the fused pyramid feature maps both in the spatial and channel-wise context. Our model is only trained on the COCO train dataset, with no extra data involved. On the COCO test-dev dataset, our single model achieves 73.6 AP with ResNet-101 backbone and 73.9 AP with ResNet-152 backbone. The ensemble model with ResNet-101 and ResNet-152 achieves 74.2 AP on the test-dev dataset and 72.8 AP on the test-challenge dataset.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.918\", \"AP\": \"0.742\", \"AP_medium\": \"0.706\", \"AR\": \"0.801\", \"AR_50\": \"0.953\", \"AP_large\": \"0.802\", \"AR_large\": \"0.86\", \"AR_75\": \"0.866\", \"AR_medium\": \"0.757\", \"AP_75\": \"0.819\"}", "leaderboard_name": "kpt-dev", "team": {"name": "ByteDance-SEU", "members": "Kai Su[1,2,*], Dongdong Yu[1,*], Zhenqi Xu[1], Xin Geng[2], Changhu Wang[1]. 1: ByteDance AI Lab, 2: Southeast University, * means Equal Contributions.", "id": 1259}, "date": "2018-09-09", "publication": ""}, {"description": "Our system is based on a top-down two stage method. Firstly, a human detector (we used Mask R-CNN) detects humans from a given input image. The detected human bounding boxes are fed to the second stage (pose estimation network). The pose estimation network is designed to be a refinement network which takes human bounding box and corresponding heatmaps for each joint and outputs refined heatmaps. We used some previous works to generate initial heatmaps. Four NVIDIA 1080ti GPUs are used for training and testing our model. No external data is used for training. We did not use model ensemble strategy.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.907\", \"AP\": \"0.738\", \"AP_medium\": \"0.705\", \"AR\": \"0.792\", \"AR_50\": \"0.947\", \"AP_large\": \"0.8\", \"AR_large\": \"0.85\", \"AR_75\": \"0.855\", \"AR_medium\": \"0.75\", \"AP_75\": \"0.81\"}", "leaderboard_name": "kpt-dev", "team": {"name": "SNU CVLAB", "members": "A list of team members: Gyeongsik Moon (Seoul National University), Ju Yong Chang (Kwangwoon University), Geonwoon Jang (Seoul National University), Kyoung Mu Lee (Seoul National University).", "id": 1260}, "date": "2018-09-09", "publication": ""}, {"description": "All the models we used in COCO Keypoints Detection Challenge 2018 were only trained on COCO2017 dataset. The input size is 384x288 for all the models. The best performance 73.0 mAP on test-dev2017 was achieved by the method of multi-context attention mechanism[1] utilized in two-staged CPN[2]. The CPN is based on ResNet-152. Our detection framework is the keypoint-only Mask R-CNN[3] based on the ResNeXt-101-FPN, whose human AP is 58.5 on test-dev2018 with SoftNMS. The final result in test-challenge2018 was the ensemble of the multi-context attentional CPN and the original CPN.", "leaderboard_id": "12", "url": "[1]https://arxiv.org/abs/1702.07432; [2]https://arxiv.org/abs/1702.07432; [3]https://arxiv.org/abs/1703.06870;", "results": "{\"AP_50\": \"0.913\", \"AP\": \"0.74\", \"AP_medium\": \"0.706\", \"AR\": \"0.802\", \"AR_50\": \"0.952\", \"AP_large\": \"0.801\", \"AR_large\": \"0.864\", \"AR_75\": \"0.867\", \"AR_medium\": \"0.757\", \"AP_75\": \"0.815\"}", "leaderboard_name": "kpt-dev", "team": {"name": "fadivugibs", "members": "Kaiyu Yue (Baidu VIS); Jiangfan Deng (Baidu VIS); Ailing Zeng (The Chinese University of HongKong); Xiao Chu (Baidu Research); Chao Li (Baidu VIS); Yi Yang (Baidu Research); Feng Zhou (Baidu Research); Shilei Wen (Baidu VIS); Ming Sun (Baidu VIS);", "id": 1261}, "date": "2018-09-09", "publication": ""}, {"description": " no any other external data\uff0cthe result i submit is single model\uff0c not ensemble", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.906\", \"AP\": \"0.724\", \"AP_medium\": \"0.686\", \"AR\": \"0.791\", \"AR_50\": \"0.948\", \"AP_large\": \"0.791\", \"AR_large\": \"0.858\", \"AR_75\": \"0.854\", \"AR_medium\": \"0.741\", \"AP_75\": \"0.797\"}", "leaderboard_name": "kpt-dev", "team": {"name": "jd_y", "members": " Boyan Zhou JD", "id": 1262}, "date": "2018-09-09", "publication": ""}, {"description": "For detection, we use four different detection models for ensemble trained on COCO 2017. For single person pose model, we use ResNet152 as backbone, following three deconvolution layers and one skip connection to predict the pose. The model trained on COCO 2017 with segmentation. Also we use structure loss and multi-scale supervision during training.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.906\", \"AP\": \"0.717\", \"AP_medium\": \"0.686\", \"AR\": \"0.77\", \"AR_50\": \"0.944\", \"AP_large\": \"0.772\", \"AR_large\": \"0.83\", \"AR_75\": \"0.836\", \"AR_medium\": \"0.728\", \"AP_75\": \"0.792\"}", "leaderboard_name": "kpt-dev", "team": {"name": "ByteCV", "members": "Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen Lu, Linfu Wen; Bytedance AI Lab", "id": 1263}, "date": "2018-09-09", "publication": ""}, {"description": "We adopt the top-down method by first detecting human and then do the single person pose estimation. We design an efficient yet accurate single person pose estimation network. Combined with our previous proposed RMPE framework (https://arxiv.org/abs/1612.00137), we are able to achieve 70+ AP without ensembling and can run on COCO test set at 20 fps. Code is publicly available at  https://github.com/MVIG-SJTU/AlphaPose", "leaderboard_id": "12", "url": "https://github.com/MVIG-SJTU/AlphaPose", "results": "{\"AP_50\": \"0.888\", \"AP\": \"0.717\", \"AP_medium\": \"0.674\", \"AR\": \"0.774\", \"AR_50\": \"0.928\", \"AP_large\": \"0.78\", \"AR_large\": \"0.841\", \"AR_75\": \"0.83\", \"AR_medium\": \"0.724\", \"AP_75\": \"0.783\"}", "leaderboard_name": "kpt-dev", "team": {"name": "Fast-20-FPS", "members": "*Jiefeng Li, *Hao-Shu Fang, Cewu Lu (* denotes equal co-first authorship);  Shanghai Jiao Tong University", "id": 1264}, "date": "2018-09-09", "publication": ""}, {"description": "MultiPoseNet, a novel bottom-up multi-person pose   estimation architecture that combines a multi-task model with a novel   assignment method. MultiPoseNet can jointly handle person detection,   keypoint detection, person segmentation and pose estimation problems.   The novel assignment method is implemented by the Pose Residual   Network (PRN) which receives keypoint and person detections, and   produces accurate poses by assigning keypoints to person instances.   Final challenge results are obtained by applying top-down refinement   over MultiPoseNet results. We use only COCO keypoints dataset to train   our models.", "leaderboard_id": "12", "url": "https://arxiv.org/abs/1807.04067", "results": "{\"AP_50\": \"0.877\", \"AP\": \"0.705\", \"AP_medium\": \"0.661\", \"AR\": \"0.749\", \"AR_50\": \"0.909\", \"AP_large\": \"0.773\", \"AR_large\": \"0.815\", \"AR_75\": \"0.807\", \"AR_medium\": \"0.701\", \"AP_75\": \"0.772\"}", "leaderboard_name": "kpt-dev", "team": {"name": "METU", "members": "Muhammed Kocabas*, Salih Karagoz*, Emre Akbas*. *:   Middle East Technical University", "id": 1251}, "date": "2018-09-09", "publication": ""}, {"description": "We train our model based on four stacked hourglass on COCO and MPII dataset, in which the targets range from coarse to fine are used for supervisions. After that, we remove other supervised information and finetune our model using only the fine target. By doing this, we get 73.1% on val2017 dataset and 69.8% on dev2017 dataset. Our final model is the ensemble of our model with Mask-RCNN [1], which helps us achieve 74.8% on the val2017 data, 71.3% on dev2017 data. Pose NMS [2] also play an important role in finding the most confident pose, merge redundant poses and redundant poses. These are helpful in improving performance on pose estimation results.     [1] He, Kaiming, et al. \"Mask r-cnn.\" Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.  [2] Fang, Haoshu, et al. \"Rmpe: Regional multi-person pose estimation.\" The IEEE International Conference on Computer Vision (ICCV). Vol. 2. 2017.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.901\", \"AP\": \"0.713\", \"AP_medium\": \"0.673\", \"AR\": \"0.761\", \"AR_50\": \"0.932\", \"AP_large\": \"0.773\", \"AR_large\": \"0.827\", \"AR_75\": \"0.819\", \"AR_medium\": \"0.713\", \"AP_75\": \"0.78\"}", "leaderboard_name": "kpt-dev", "team": {"name": "Raven-DL", "members": "Lianbo Zhang, University of Technology Sydney Jue Wang, University of Technology Sydney", "id": 1266}, "date": "2018-09-09", "publication": ""}, {"description": "We develop a novel bottom-up pose estimation method, based on an improved version of stacked hourglass network with an associative embedding loss. Our model can accomplish keypoint localization and grouping at the same time. We train our model without any extra data. Our single model achieves 0.684 AP on test-challenge and 0.702 AP on test-dev with multi-scale testing.  No model ensemble is used.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.892\", \"AP\": \"0.702\", \"AP_medium\": \"0.656\", \"AR\": \"0.747\", \"AR_50\": \"0.914\", \"AP_large\": \"0.763\", \"AR_large\": \"0.821\", \"AR_75\": \"0.806\", \"AR_medium\": \"0.693\", \"AP_75\": \"0.77\"}", "leaderboard_name": "kpt-dev", "team": {"name": "TFMAN", "members": "Xinyu Gong, Weihang Chen", "id": 1267}, "date": "2018-09-09", "publication": ""}, {"description": "Method description: we use resnetv2 as the backbone convolotional network to provide four pyramid group of features (outputs of endpoints 'block1','block2','block3','block4') , we regress keypoints heatmaps from each group of features independently by connecting further sequence of convolutional and upsample operations. The four sets of heatmaps resulting from differnt pyramid features are finally added together to serve as the final result. we only use data in person_keypoints_train2017, the single model best performance on test-dev is 0.65, no emsemble trick of multi models are used.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.87\", \"AP\": \"0.654\", \"AP_medium\": \"0.609\", \"AR\": \"0.7\", \"AR_50\": \"0.9\", \"AP_large\": \"0.722\", \"AR_large\": \"0.771\", \"AR_75\": \"0.755\", \"AR_medium\": \"0.648\", \"AP_75\": \"0.717\"}", "leaderboard_name": "kpt-dev", "team": {"name": "Huya", "members": "Li Baishun", "id": 1268}, "date": "2018-09-09", "publication": ""}, {"description": "Human pose estimation is a fundamental research topic in computer vision. This topic has been largely improved recently thanks to the development of convolution neural network. This paper proposes a new CNN architecture which combines the key-points estimators and an object detector. This network can detect poses of all people and the around objects in the image in parallel. In general, to address the multi-person pose estimation, the network generates human key-point heatmaps and bounding boxes simultaneously. Then, for each person-bounding-box, it crops image and the heatmaps based on the coordinate. These cropped images are passed through another single-person pose estimator to generate the pose of person inside the bounding box. The proposed network is just trained on the COCO train dataset with augmentation.  The proposed network is implemented based on mxnet, run on a desktop with GTX 1080Ti devices. It takes 0.244 seconds for generating heatmaps and bounding boxes, and 0.1 seconds for estimating pose of single person.", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": \"0.765\", \"AP\": \"0.522\", \"AP_medium\": \"0.542\", \"AR\": \"0.589\", \"AR_50\": \"0.804\", \"AP_large\": \"0.575\", \"AR_large\": \"0.635\", \"AR_75\": \"0.634\", \"AR_medium\": \"0.556\", \"AP_75\": \"0.571\"}", "leaderboard_name": "kpt-dev", "team": {"name": "UOU_ISLab", "members": "Van-Thanh Hoang, Intelligent System Lab., School of Electrical Engineering, University of Ulsan, Ulsan, Korea, Kang-Hyun Jo, Intelligent System Lab., School of Electrical Engineering, University of Ulsan, Ulsan, Korea", "id": 1269}, "date": "2018-09-09", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.905, \"AR_medium\": 0.733, \"AR_50\": 0.945, \"AP_large\": 0.776, \"AR_75\": 0.548, \"AP\": 0.713, \"AP_medium\": 0.677, \"AR\": 0.780, \"AR_large\": 0.843, \"AP_75\": 0.787}", "results_details": "", "leaderboard_id": "12", "team": {"id": 1270, "members": "", "name": "houluanxuan"}, "date": "2019-08-02", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.862, \"AR_medium\": 0.682, \"AR_50\": 0.886, \"AP_large\": 0.693, \"AR_75\": 0.768, \"AP\": 0.674, \"AP_medium\": 0.668, \"AR\": 0.717, \"AR_large\": 0.765, \"AP_75\": 0.732}", "results_details": "", "leaderboard_id": "12", "team": {"id": 1271, "members": "", "name": "Secrete-KP"}, "date": "2019-07-24", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.935, \"AR_medium\": 0.794, \"AR_50\": 0.964, \"AP_large\": 0.833, \"AR_75\": 0.899, \"AP\": 0.781, \"AP_medium\": 0.747, \"AR\": 0.831, \"AR_large\": 0.881, \"AP_75\": 0.859}", "results_details": "", "leaderboard_id": "12", "team": {"id": 1272, "members": "", "name": "jzha7896"}, "date": "2019-07-24", "publication": ""}, {"publication": "", "leaderboard_id": "12", "url": "", "results": "{\"AP_50\": 0.919, \"AR_medium\": 0.751, \"AR_50\": 0.954, \"AP_large\": 0.800, \"AR_75\": 0.863, \"AP\": 0.739, \"AP_medium\": 0.706, \"AR\": 0.792, \"AR_large\": 0.849, \"AP_75\": 0.819}", "leaderboard_name": "kpt-dev", "team": {"id": 2581, "members": "", "name": "MCC"}, "date": "2019-07-18", "result_details": "", "description": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/DarkPose.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "https://ilovepose.github.io/coco/", "results": "{  \"AP\": \"0.789\", \"AP_50\": \"0.938\",\"AP_75\": \"0.860\",    \"AP_medium\": \"0.751\",    \"AP_large\": \"0.844\",     \"AR\": \"0.835\",    \"AR_50\": \"0.967\", \"AR_75\": \"0.899\",    \"AR_medium\": \"0.797\",    \"AR_large\": \"0.888\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1271, "members": "Hanbin Dai[1,*] Liangbo Zhou[1,*] Feng Zhang[1,*] Zhengyu Zhang[2,*] Hong Hu[1,*]  Xiatian Zhu[3,*] Mao Ye [1] 1: University of Electronic Science and Technology of China; 2: Shenzhen University; 3: University of Surrey; * means equal contribution.", "name": "DarkPose"}, "date": "2019-10-27", "publication": ""}, {"description": "We adopt a two-stage top-down pipeline: first detect human bounding boxes using HRNetV2p-W48 with Hybrid Task Cascade architecture (59.6 AP for the person category on the COCO validation set) and then perform single person pose estimation on the detected bounding boxes. The pose estimator is our own extended version of HRNet, which has SE-residual unit, skip-connection across modules and modified head architecture. We first pretrain the model with MSE over keypoint heatmaps and then fine-tune the model using L1 loss for regressed keypoint positions obtained by soft-argmax operation on the heatmaps to improve localization accuracy. AI Challenger dataset is used for pretraining. Our single model achieves 75.8 AP on the test-dev set. Our final ensemble results averaging predicted coordinates by the pretrained heatmap model and the fine-tuned regression model achieves 75.9 AP on the test-dev set and 73.1 AP on the test-challenge set. See <a href=\"../files/keypoints_2019_reports/Dproject.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.759\",  \"AP_50\": \"0.918\",  \"AP_75\": \"0.831\",  \"AP_medium\": \"0.721\",  \"AP_large\": \"0.822\",  \"AR\": \"0.809\",  \"AR_50\": \"0.953\",  \"AR_75\": \"0.873\",  \"AR_medium\": \"0.765\",  \"AR_large\": \"0.869\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1272, "members": "Naoki Kato, Hideki Okada, Yusuke Uchida (all members are belonging to DeNA Co., Ltd.)", "name": "Dproject"}, "date": "2019-10-27", "publication": ""}, {"description": "We use a two stage top-down method. In the first stage, we use Hybrid Task Cascade to detect human bounding boxes. In the second stage, we use our Structured Fusion Network with Deconvolution Head and two-stage network to detect human keypoints. Finally we use Refinement Network to refine the results got by using ensemble method(5 models). We train models using extra data (AI-Challenger dataset) . Our best single model has 0.764 of mAP on COCO test-dev dataset, and final result has 0.775 of mAP on COCO test-dev dataset and 0.751 of mAP on COCO test-challenge dataset. See <a href=\"../files/keypoints_2019_reports/NeteaseGameAI.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.775\",  \"AP_50\": \"0.927\",  \"AP_75\": \"0.848\",  \"AP_medium\": \"0.743\",  \"AP_large\": \"0.831\",  \"AR\": \"0.823\",  \"AR_50\": \"0.959\",  \"AR_75\": \"0.888\",  \"AR_medium\": \"0.782\",  \"AR_large\": \"0.879\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1273, "members": "Jia Wei Netease Game AI Lab weijia@corp.netease.com, Yanjun Li Netease Game AI Lab liyanjun@corp.netease.com", "name": "NeteaseGameAI"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Megvii.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.792\",  \"AP_50\": \"0.944\",  \"AP_75\": \"0.871\",  \"AP_medium\": \"0.761\",  \"AP_large\": \"0.838\",  \"AR\": \"0.841\",  \"AR_50\": \"0.968\",  \"AR_75\": \"0.907\",  \"AR_medium\": \"0.808\",  \"AR_large\": \"0.885\"}", "results_details": {"ensemble": true, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1274, "members": "Yuanhao Cai*, Zhicheng Wang*, Binyi Yin, Ruihao Yin, Angang Du, Zhengxiong Luo, Zeming Li, Xinyu Zhou, Gang Yu, Erjin Zhou, Xiangyu Zhang, Yichen Wei, Jian Sun (* indicates equal contribution) Affiliations: Megvii Research", "name": "Megvii"}, "date": "2019-10-27", "publication": ""}, {"description": "We built a single-stage system, which estimates bounding boxes and corresponding keypoints in a single model. Based on Mask R-CNN, we develop a novel ROI feature extraction method using channel attention. Combined with multi-scale keypoint head network, our model achieve 57.2AP for human detection and 70.3AP for keypoint on COCO 2017 validation set with ResNet50-FPN backbone. No external data is used for training. We did not use model ensemble strategy. See <a href=\"../files/keypoints_2019_reports/MscaRCNN.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.688\",  \"AP_50\": \"0.900\",  \"AP_75\": \"0.756\",  \"AP_medium\": \"0.643\",  \"AP_large\": \"0.766\",  \"AR\": \"0.755\",  \"AR_50\": \"0.942\",  \"AR_75\": \"0.816\",  \"AR_medium\": \"0.704\",  \"AR_large\": \"0.824\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1275, "members": "Myunggu Kang, Clova AI, NAVER Corp (myunggu.kang@navercorp.com) Dongyoon Wee, Clova AI, NAVER Corp (dongyoon.wee@navercorp.com)", "name": "MscaRCNN"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/ByteDanceVC.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.787\",  \"AP_50\": \"0.935\",  \"AP_75\": \"0.859\",  \"AP_medium\": \"0.753\",  \"AP_large\": \"0.846\",  \"AR\": \"0.841\",  \"AR_50\": \"0.965\",  \"AR_75\": \"0.902\",  \"AR_medium\": \"0.800\",  \"AR_large\": \"0.800\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1276, "members": "Dongdong Yu[1,*], Kai Su[1,*], Changhu Wang[1]; 1: ByteDance AI Lab, * means Equal Contributions.", "name": "ByteDanceVC"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/PifPaf.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "https://github.com/vita-epfl/openpifpaf", "results": "{\"AP\": \"0.712\", \"AP_50\": \"0.894\", \"AP_75\": \"0.785\", \"AP_medium\": \"0.674\", \"AP_large\": \"0.769\", \"AR\": \"0.773\", \"AR_50\": \"0.928\", \"AR_75\": \"0.833\", \"AR_medium\": \"0.720\", \"AR_large\": \"0.845\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1277, "members": "Sven Kreiss, Alexandre Alahi, EPFL", "name": "PifPaf"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/KakaoBrain.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.777\", \"AP_50\": \"0.930\", \"AP_75\": \"0.850\", \"AP_medium\": \"0.740\", \"AP_large\": \"0.830\", \"AR\": \"0.821\", \"AR_50\": \"0.960\", \"AR_75\": \"0.886\", \"AR_medium\": \"0.785\", \"AR_large\": \"0.873\"}", "results_details": {"ensemble": true, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1278, "members": "Sanghoon Hong, Kakao Brain, sanghoon.hong@kakaobrain.com, Hunchul Park, Kakao Brain, robert.p@kakaobrain.com, Jonghyuk Park, Seoul National University, chico2121@snu.ac.kr, Sukhyun Cho, Seoul National University, chosh90@snu.ac.kr, Heewoong Park, Seoul National University, hee188@snu.ac.kr", "name": "KakaoBrain"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Usaic.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.789\", \"AP_50\": \"0.938\", \"AP_75\": \"0.860\", \"AP_medium\": \"0.749\", \"AP_large\": \"0.845\", \"AR\": \"0.834\", \"AR_50\": \"0.963\", \"AR_75\": \"0.898\", \"AR_medium\": \"0.795\", \"AR_large\": \"0.888\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1279, "members": "Jing Zhang, Zhe Chen and Dacheng Tao, UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia", "name": "Usaic"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Eleme.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.765\", \"AP_50\": \"0.929\", \"AP_75\": \"0.838\", \"AP_medium\": \"0.728\", \"AP_large\": \"0.824\", \"AR\": \"0.813\", \"AR_50\": \"0.958\", \"AR_75\": \"0.878\", \"AR_medium\": \"0.771\", \"AR_large\": \"0.870\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1280, "members": "Shuchun Liu, Feiyun Zhang, Li Long, Weiyuan Shao, Jiajun Wang", "name": "Eleme"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/ByteDanceHRNet.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch", "results": "{\"AP\": \"0.782\", \"AP_50\": \"0.928\", \"AP_75\": \"0.855\", \"AP_medium\": \"0.748\", \"AP_large\": \"0.841\", \"AR\": \"0.828\", \"AR_50\": \"0.959\", \"AR_75\": \"0.891\", \"AR_medium\": \"0.789\", \"AR_large\": \"0.882\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1281, "members": "Bin Xiao (ByteDance), Zaizhou Gong (ByteDance), Yifan Lu (ByteDance), Linfu Wen (ByteDance)", "name": "ByteDanceHRNet"}, "date": "2019-10-27", "publication": "https://arxiv.org/abs/1902.09212, https://arxiv.org/abs/1908.07919"}, {"description": " See <a href=\"../files/keypoints_2019_reports/HigherHRNet.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation", "results": "{\"AP\": \"0.721\", \"AP_50\": \"0.895\", \"AP_75\": \"0.784\", \"AP_medium\": \"0.681\", \"AP_large\": \"0.775\", \"AR\": \"0.761\", \"AR_50\": \"0.910\", \"AR_75\": \"0.818\", \"AR_medium\": \"0.712\", \"AR_large\": \"0.831\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1282, "members": "Bowen Cheng (UIUC), Bin Xiao (ByteDance), Jingdong Wang (Microsoft), Honghui Shi (UIUC, University of Oregon), Thomas S. Huang (UIUC), Lei Zhang (Microsoft)", "name": "HigherHRNet"}, "date": "2019-10-27", "publication": "https://arxiv.org/abs/1908.10357"}, {"description": " See <a href=\"../files/keypoints_2019_reports/EditC.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.753\", \"AP_50\": \"0.927\", \"AP_75\": \"0.832\", \"AP_medium\": \"0.719\", \"AP_large\": \"0.811\", \"AR\": \"0.802\", \"AR_50\": \"0.958\", \"AR_75\": \"0.872\", \"AR_medium\": \"0.762\", \"AR_large\": \"0.858\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-dev", "team": {"id": 1283, "members": "Zixuan Huang huangzixuan0508@bupt.edu.cn, Yuan Chang changyuan@bupt.edu.cn", "name": "EditC"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Casia.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.777\", \"AP_50\": \"0.925\", \"AP_75\": \"0.850\", \"AP_medium\": \"0.743\", \"AP_large\": \"0.834\", \"AR\": \"0.830\", \"AR_50\": \"0.959\", \"AR_75\": \"0.892\", \"AR_medium\": \"0.789\", \"AR_large\": \"0.885\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1284, "members": "Junjie Huang (Institute of Automation, Chinese Academy of Sciences); Zheng Zhu (Institute of Automation, Chinese Academy of Sciences); Guan Huang (Institute of Automation, Chinese Academy of Sciences)", "name": "Casia"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Infinova.pdf\">technical report</a>. ", "leaderboard_id": 12, "url": "", "results": "{\"AP\": \"0.755\", \"AP_50\": \"0.926\", \"AP_75\": \"0.834\", \"AP_medium\": \"0.719\", \"AP_large\": \"0.815\", \"AR\": \"0.804\", \"AR_50\": \"0.957\", \"AR_75\": \"0.873\", \"AR_medium\": \"0.763\", \"AR_large\": \"0.861\"}", "results_details": {"ensemble": false, "external_data": true}, "leaderboard_name": "kpt-dev", "team": {"id": 1285, "members": "Zhang Fangjian, Cheng Wei, Li Liang, Li Liuwu", "name": "Infinova"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/keypoints/UDP.pdf\">technical report</a>. ", "leaderboard_name": "kpt-dev", "url": "https://github.com/HuangJunJie2017/UDP-Pose","results": "{\"AP\": \"0.808\", \"AP_50\": \"0.949\", \"AP_75\": \"0.881\", \"AP_medium\": \"0.774\", \"AP_large\": \"0.857\", \"AR\": \"0.853\", \"AR_50\": \"0.973\", \"AR_75\": \"0.915\", \"AR_medium\": \"0.817\", \"AR_large\": \"0.901\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 12, "team": {"id": 1286, "members": "Junjie Huang*, Zengguang Shan*, Yuanhao Cai*, Feng Guo, Yun Ye and Xinze Chen (XForwardAI Technology, * indicates equal contribution), Zheng Zhu (Tsinghua University), Guan Huang (XForwardAI Technology), Jiwen Lu (Tsinghua University), Dalong Du (XForwardAI Technology)", "name": "XForwardAI"}, "date": "2020-08-23", "publication": ""}, {"description": "We use a two stage top-down method. In the first stage, we use DetectoRS to detect human bounding boxes. In the second stage, we use Structured Fusion Network (SFNetv2) to detect human keypoints. Finally we use Refinement Network to refine the results by using ensemble method with 6 models. Our best single model has 0.775 of mAP on COCO test-dev dataset, andfinal result has 0.781 of mAP on COCO test-dev dataset and 0.755 of mAP on COCO test-challenge dataset. See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/keypoints/SFNet.pdf\">technical report</a>.","leaderboard_name": "kpt-dev","url": "","results": "{\"AP\": \"0.781\", \"AP_50\": \"0.931\", \"AP_75\": \"0.854\", \"AP_medium\": \"0.747\", \"AP_large\": \"0.841\", \"AR\": \"0.828\", \"AR_50\": \"0.962\", \"AR_75\": \"0.893\", \"AR_medium\": \"0.789\", \"AR_large\": \"0.883\"}","results_details": {"ensemble": true, "external_data": true},"leaderboard_id": 12,"team": {"id": 1287, "members": "Li Yanjun; Zhifan Li; Jia Wei", "name": "Netease"},"date": "2020-08-23","publication": ""},{"description": "This paper is the technical report of team ReferAlgo for COCO Keypoint Challenge 2020. We follow the top-down paradigm in this contest, which firstly applies detectoRS to detect persons from given images, and then estimates keypoints of detected human bodies with HRNet-based methods. Our best single model achieved AP score of 77.8 on COCO test-dev dataset, and the performance of our ensembled methods reached 78.3 on COCO test-dev dataset and 75.9 on COCO test-challenge dataset respectively. See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/keypoints/ReferAlgo.pdf\">technical report</a>.","leaderboard_name": "kpt-dev","url": "","results": "{\"AP\": \"0.778\", \"AP_50\": \"0.935\", \"AP_75\": \"0.851\", \"AP_medium\": \"0.744\", \"AP_large\": \"0.835\", \"AR\": \"0.823\", \"AR_50\": \"0.961\", \"AR_75\": \"0.888\", \"AR_medium\": \"0.785\", \"AR_large\": \"0.875\"}","results_details": {"ensemble": true, "external_data": true},"leaderboard_id": 12,"team": {"id": 1288, "members": "Min Du (Horizon Robotics); Kun Zhang (Institute of Computing Technology, Chinese Academy of Sciences); Yuhao Dou (Horizon Robotics); Rui Wu (Horizon Robotics)", "name": "ReferAlgo"},"date": "2020-08-23","publication": ""}]
