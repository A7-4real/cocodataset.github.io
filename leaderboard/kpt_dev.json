[{"description": "A single network for multi-person keypoint localization based on the stacked hourglass design (Newell et al. ECCV 2016). No additional training data or separate person detection is used. For each body joint at each pixel location, our network predicts a detection score, plus a scalar score that serves as a \u201ctag\u201d. Body joints with similar tags are grouped together to form individual persons. The detection scores and tags are jointly trained.", "leaderboard_name": "kpt-dev", "url": "https://github.com/anewell/pose-hg-train", "results": "{\"AP_50\": 0.746, \"AR_medium\": 0.407, \"AR_50\": 0.771, \"AP_large\": 0.556, \"AR_75\": 0.546, \"AP\": 0.46, \"AP_medium\": 0.388, \"AR\": 0.518, \"AR_large\": 0.669, \"AP_75\": 0.484}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1235, "members": "Alejandro Newell, Jia Deng", "name": "umich_vl"}, "date": "2016-09-16", "publication": "https://arxiv.org/abs/1603.06937"}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.295, \"AR_medium\": 0.105, \"AR_50\": 0.43, \"AP_large\": 0.137, \"AR_75\": 0.142, \"AP\": 0.103, \"AP_medium\": 0.081, \"AR\": 0.188, \"AR_large\": 0.3, \"AP_75\": 0.054}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1236, "members": "", "name": "zjhuang"}, "date": "2016-09-16", "publication": ""}, {"description": "We present a bottom-up method that first detects individual human joints and then connects joints to form bodies using a limb affinity score. Both the joint detections and limb affinity scores are predicted jointly by a fully convolutional neural network, based on the iterated prediction architecture of convolutional pose machines. A greedy algorithm then connects limbs into subsets to form individual person detections. The system was trained only on MSCOCO data.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.849, \"AR_medium\": 0.606, \"AR_50\": 0.872, \"AP_large\": 0.682, \"AR_75\": 0.718, \"AP\": 0.618, \"AP_medium\": 0.571, \"AR\": 0.665, \"AR_large\": 0.746, \"AP_75\": 0.675}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1237, "members": "Zhe Cao, Shih-En Wei, Tomas Simon, Yaser Sheikh", "name": "CMU-Pose"}, "date": "2016-09-16", "publication": ""}, {"description": "The G-RMI detection entry is a two-stage system consisting of a bounding box person detector followed by a human pose estimator. For person bounding box detection we use a combination of G-RMI's generic ResNet-Inception-based object detector trained only on MS-COCO data, as well as a person-specific box detector trained on both MS-COCO and in-house data. For pose estimation we use a single ResNet-101 based model trained to predict dense keypoint heatmap and offset fields, whose fusion yields highly localized keypoint activation maps which are used for both keypoint localization and proposal rescoring. The pose estimator has been trained on both MS-COCO and in-house data.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.822, \"AR_medium\": 0.619, \"AR_50\": 0.866, \"AP_large\": 0.666, \"AR_75\": 0.714, \"AP\": 0.605, \"AP_medium\": 0.576, \"AR\": 0.662, \"AR_large\": 0.722, \"AP_75\": 0.662}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1238, "members": "Nori Kanazawa*, George Papandreou*, Alex Toshev*, Tyler Zhu*, Hartwig Adam, Chris Bregler, Kevin Murphy, Jonathan Tompson, (alphabetical order, core system developers marked with asterisk)", "name": "G-RMI_2016"}, "date": "2016-09-16", "publication": ""}, {"description": "We have implemented a \u00ab specialist model \u00bb that specializes its last layes on specific body parts. To do so we have stacked in parrallel, block of convolutional layers on features map of a pertained model (VGG16), and we have jointly trained these regressor on the task of keypoint detection.", "leaderboard_name": "kpt-dev", "url": "https://github.com/NaasCraft/SSR_COCO", "results": "{\"AP_50\": 0.084, \"AR_medium\": 0.069, \"AR_50\": 0.256, \"AP_large\": 0.02, \"AR_75\": 0.022, \"AP\": 0.017, \"AP_medium\": 0.018, \"AR\": 0.072, \"AR_large\": 0.077, \"AP_75\": 0.001}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1239, "members": "Raphael Canyasse, Guillaume Demonet, Charles Sutton", "name": "USSR"}, "date": "2016-09-16", "publication": ""}, {"description": "1. Use faster RCNN to generate a lot of bounding boxes.  2. Use single-person pose estimator to generate keypoints regression.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.75, \"AR_medium\": 0.499, \"AR_50\": 0.77, \"AP_large\": 0.567, \"AR_75\": 0.61, \"AP\": 0.514, \"AP_medium\": 0.474, \"AR\": 0.563, \"AR_large\": 0.649, \"AP_75\": 0.559}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1240, "members": "Yuxiang Peng, Gang Yu", "name": "R4D"}, "date": "2016-09-16", "publication": ""}, {"description": "The main idea of our method is we first samples a small number of representable pixels and then determine their labels via a convolutional layer followed by a softmax layer. We develop a CNN architecture to predict the key points from different layers, then fuse all the side predictions as the final prediction.", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.753, \"AR_medium\": 0.753, \"AR_50\": 0.827, \"AP_large\": 0.543, \"AR_75\": 0.692, \"AP\": 0.544, \"AP_medium\": 0.583, \"AR\": 0.708, \"AR_large\": 0.768, \"AP_75\": 0.509}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1241, "members": "Shaoli Huang, Dacheng Tao", "name": "DL-61"}, "date": "2016-09-16", "publication": ""}, {"description": "", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.617, \"AR_medium\": 0.316, \"AR_50\": 0.63, \"AP_large\": 0.58, \"AR_75\": 0.489, \"AP\": 0.419, \"AP_medium\": 0.3, \"AR\": 0.454, \"AR_large\": 0.639, \"AP_75\": 0.452}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1242, "members": "", "name": "belagian"}, "date": "2016-09-16", "publication": ""}, {"description": "A pipeline approach with Multibox [1] to detect people and Stacked Hourglass Networks [2] to estimate their pose. [1] Scalable High Quality Object Detection. Erhan et al., CVPR 2014 [2] Stacked Hourglass Networks for Human Pose Estimation. Newell et al., POCV 2016", "leaderboard_name": "kpt-dev", "url": "", "results": "{\"AP_50\": 0.652, \"AR_medium\": 0.448, \"AR_50\": 0.8, \"AP_large\": 0.492, \"AR_75\": 0.566, \"AP\": 0.402, \"AP_medium\": 0.349, \"AR\": 0.538, \"AR_large\": 0.659, \"AP_75\": 0.419}", "results_details": "", "leaderboard_id": 12, "team": {"id": 1243, "members": "Grant Van Horn, Matteo Ruggero Ronchi, David Hall", "name": "Caltech"}, "date": "2016-09-16", "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "Our keypoint results were obtained by single-person skeleton models, on the human bounding boxes generated by MegDet, a large-batch object detector by Megvii (Face++). The single-person skeleton model is based on a ResNet with U-shape side branch to integrate high resolution information from the earlier layers and semantic information from the deeper layers. Large kernel [1] convolution was utilized to enlarge the effective receptive field. Ensemble was built on the feature-map level to generate the final results. Our method was trained on the provided labeled images only.", "leaderboard_id": "12", "url": "https://arxiv.org/abs/1711.07319", "team": {"id": 1244, "members": "Yilun Chen*, Zhicheng Wang*, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Jian Sun (* indicates equal contribution); Megvii Research", "name": "Megvii (Face++)"}, "results": "{\"AP_50\": 0.917, \"AR_medium\": 0.748, \"AR_50\": 0.951, \"AP_large\": 0.781, \"AR_75\": 0.859, \"AP\": 0.730, \"AP_medium\": 0.695, \"AR\": 0.790, \"AR_large\": 0.846, \"AP_75\": 0.809}", "results_details": {"ensemble": true, "external_data": false}, "publication": "https://arxiv.org/abs/1703.02719"}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "We implement a two stage top-down method. In the first stage, we use mask rcnn like framework to detect human bounding boxes under both supervision from detection and keypoints. In the second stage, we perform single person pose estimation. In addition, we remove false annotated data based on the keypoint annotation and detection annotation jointly. We also use both detection and keypoint score for instance re-scoring and perform box-nms and oks-nms based on it. In both stages we use only single model, no model ensembles are applied. Additional data are from a public dataset of AI-Challenger keypoints detection competition track and provide 1.5 point AP improvement on the validation dataset.", "leaderboard_id": "12", "url": "https://github.com/MVIG-SJTU/RMPE", "team": {"id": 1245, "members": "Changbao Wang (Beihang University), Yujie Wang (Beihang University), Quanquan Li (SenseTime Group Limited)", "name": "oks"}, "results": "{\"AP_50\": 0.903, \"AR_medium\": 0.725, \"AR_50\": 0.939, \"AP_large\": 0.784, \"AR_75\": 0.840, \"AP\": 0.720, \"AP_medium\": 0.676, \"AR\": 0.771, \"AR_large\": 0.835, \"AP_75\": 0.797}", "results_details": {"ensemble": false, "external_data": true}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "We build a two-stage system consisting of a bounding-box person detector followed by a human pose estimator. For person detection, we train a faster rcnn with deformable version of resnet101 on MS-COCO trainval2017. Multi-scale testing and soft-NMS are used. For pose estimation, we train a resnet-101 based model to predict heatmap and offset fields of keypoints for the person in the detected bounding box. The performance is boosted by OKS-based NMS in post-processing. The pose estimator is trained using COCO MS-trainval2017 and extra dataset. For single model, we obtain an average precision of 0.713 on the test-dev set and 0.688 on the test-challenge set. Finally, we use an ensemble of seven models and obtain an average precision of 0.728 on the test-dev set and 0.706 on the test-challenge set.", "leaderboard_id": "12", "url": "", "team": {"id": 1246, "members": "Xinze Chen*, Rui Wu*, Yun Ye and Guan Huang (*The first two authors contribute equally to this work)", "name": "bangbangren"}, "results": "{\"AP_50\": 0.894, \"AR_medium\": 0.736, \"AR_50\": 0.941, \"AP_large\": 0.800, \"AR_75\": 0.848, \"AP\": 0.728, \"AP_medium\": 0.686, \"AR\": 0.787, \"AR_large\": 0.856, \"AP_75\": 0.796}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "Our method is based on the two-stage approach described in our CVPR 2017 paper (https://arxiv.org/abs/1701.01779), with some improvements (use of a Resnet-152 instead of a ResNet-101 backbone, better feature alignment, more refined training schedule, etc). Our final entry in the competition uses a single model (no ensembling). In addition to the COCO images and annotations (and Imagenet image classification pretraining), we also used for training our model an internal dataset of images with keypoint annotations which is described in detail in Sec. 4.1 of our paper (same number of extra annotations as our last year's entry to the competition). These extra data improved keypoints test-dev AP by 3-4%.", "leaderboard_id": "12", "url": "", "team": {"id": 1247, "members": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Hartwig Adam, Kevin Murphy (Google Research)", "name": "G-RMI"}, "results": "{\"AP_50\": 0.879, \"AR_medium\": 0.714, \"AR_50\": 0.912, \"AP_large\": 0.752, \"AR_75\": 0.819, \"AP\": 0.710, \"AP_medium\": 0.690, \"AR\": 0.758, \"AR_large\": 0.820, \"AP_75\": 0.777}", "results_details": {"ensemble": false, "external_data": true}, "publication": "https://arxiv.org/abs/1701.01779"}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "Our keypoint entry is build upon the several papers developed at FAIR: (i) Mask R-CNN [1], for bbox and keypoint prediction; (ii) Feature Pyramid Networks (FPN) [2], and (iii) ResNeXt [3] 101-32x8d pre-trained on ImageNet-5k as in [3], as the backbone. In this entry, we highlight our newly developed large-scale semi-supervised learning paradigm which we call Data Distillation [4, to appear]. We apply Mask R-CNN with test-time augmentation on a large set of unlabeled images (Sports-1M frames without using any annotations), and we view these predictions as new annotations which we combine with the COCO images to retrain our model. No data labeled with human boxes, masks, or keypoints, beyond what is provided in COCO, are used in our entry. Main results on test-dev include: (i) Single-model, Mask R-CNN baseline, with FPN and ResNeXt-101-32x8d: 65.8 keypoint AP (ii) Single-model, plus Data Distillation from unlabeled Sports-1M: 67.0 keypoint AP  (iii) Single-model, Data Distillation plus test time augmentations (horizontal flip, multi-scale): 69.2 keypoint AP [1] https://arxiv.org/abs/1703.06870 [2] https://arxiv.org/abs/1612.03144 [3] https://arxiv.org/abs/1611.05431 [4] in preparation.", "leaderboard_id": "12", "url": "", "team": {"id": 1248, "members": "*Georgia Gkioxari, *Ilija Radosavovic, Kaiming He, Ross Girshick, Piotr Dollár (* denotes equal co-first authorship); Facebook AI Research", "name": "FAIR Mask R-CNN"}, "results": "{\"AP_50\": 0.904, \"AR_medium\": 0.703, \"AR_50\": 0.937, \"AP_large\": 0.763, \"AR_75\": 0.811, \"AP\": 0.692, \"AP_medium\": 0.649, \"AR\": 0.752, \"AR_large\": 0.818, \"AP_75\": 0.760}", "results_details": {"ensemble": false, "external_data": false}, "publication": "https://arxiv.org/abs/1703.06870"}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "We propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections. We train our single person pose estimator on both COCO and AI challenger dataset. Our single model performance achieved 68.5 mAP on test-dev set. We were able to achieve 69.7 mAP on test-dev set by fusing 3 models after the competition. The inference time for an image is around 1-2s.", "leaderboard_id": "12", "url": "", "team": {"id": 1249, "members": "Hao-Shu Fang, Shanghai Jiao Tong University; Jiefeng Li, Shanghai Jiao Tong University; Ruiheng Chang, Shanghai Jiao Tong University; Jinkun Cao, Shanghai Jiao Tong University; Yinghong Fang, Shanghai Jiao Tong University; Yu-Wing Tai, Tencent YouTu; Cewu Lu(*corresponding author), Shanghai Jiao Tong University", "name": "SJTU"}, "results": "{\"AP_50\": 0.875, \"AR_medium\": 0.689, \"AR_50\": 0.910, \"AP_large\": 0.751, \"AR_75\": 0.798, \"AP\": 0.688, \"AP_medium\": 0.646, \"AR\": 0.736, \"AR_large\": 0.802, \"AP_75\": 0.759}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "The submitted results are obtained by using the 'divide and combine' technique and AE method [1]. [1]Associative Embedding: End-to-End Learning for Joint Detection and Grouping Newell et al. NIPS 2017.", "leaderboard_id": "12", "url": "", "team": {"id": 1250, "members": "Yue Liao, Yao Sun, Si Liu, Yinglu Liu, Yanli Li, Junjun Xiong.", "name": "iie-samsung-pose"}, "results": "{\"AP_50\": 0.852, \"AR_medium\": 0.626, \"AR_50\": 0.885, \"AP_large\": 0.713, \"AR_75\": 0.741, \"AP\": 0.636, \"AP_medium\": 0.582, \"AR\": 0.688, \"AR_large\": 0.774, \"AP_75\": 0.698}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "METU is a pipeline approach that combines a person detector (PD) with a single person pose estimator. The erroneous detection from PD was corrected via adaptive thresholding of confidence results.", "leaderboard_id": "12", "url": "", "team": {"id": 1251, "members": "M. Salih Karagoz, Muhammed Kocabas.", "name": "METU"}, "results": "{\"AP_50\": 0.678, \"AR_medium\": 0.405, \"AR_50\": 0.720, \"AP_large\": 0.507, \"AR_75\": 0.548, \"AP\": 0.412, \"AP_medium\": 0.355, \"AR\": 0.484, \"AR_large\": 0.591, \"AP_75\": 0.452}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "", "leaderboard_id": "12", "url": "", "team": {"id": 1252, "members": "", "name": "Jessie33321"}, "results": "{\"AP_50\": 0.720, \"AR_medium\": 0.473, \"AR_50\": 0.781, \"AP_large\": 0.548, \"AR_75\": 0.606, \"AP\": 0.451, \"AP_medium\": 0.383, \"AR\": 0.560, \"AR_large\": 0.679, \"AP_75\": 0.476}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "", "leaderboard_id": "12", "url": "", "team": {"id": 1253, "members": "", "name": "Gnxr9"}, "results": "{\"AP_50\": 0.667, \"AR_medium\": 0.566, \"AR_50\": 0.833, \"AP_large\": 0.354, \"AR_75\": 0.663, \"AP\": 0.411, \"AP_medium\": 0.519, \"AR\": 0.620, \"AR_large\": 0.694, \"AP_75\": 0.424}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_name": "kpt-dev", "date": "2017-10-29", "description": "", "leaderboard_id": "12", "url": "", "team": {"id": 1253, "members": "", "name": "LwhL"}, "results": "{\"AP_50\": 0.550, \"AR_medium\": 0.416, \"AR_50\": 0.767, \"AP_large\": 0.295, \"AR_75\": 0.548, \"AP\": 0.301, \"AP_medium\": 0.366, \"AR\": 0.524, \"AR_large\": 0.670, \"AP_75\": 0.285}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}]