[
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-18",
    "description": "We train stuff segmentation model and instance segmentation model separately and merge the results. For the stuff segmentation, we enhance PSPNet [1] with a backbone network by larger dilation rate and more residual blocks to enlarge the effective receptive field. In addition, we jointly train COCO stuff and instance to get more context information. Our single model achieved 54.1 mIOU on the validation set. An ensemble of three models boosted the accuracy to 55.9. We use 8 V100 (16G memory) GPU cards. For the instance segmentation, we adopt a two-pass pipeline. Please see more details in our instance segmentation submission. Our final panoptic segmentation result is 53.2 PQ on test-dev, by merging the stuff result (55.9 mIOU in the validation set) and instance result (48.8 mmAP on test-dev). We use ShuffleNet V2 as the backbone network of above two tasks. [1] Pyramid Scene Parsing Network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia. CVPR 2017. [2] ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design, Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun, ECCV 2018.",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 1,
      "members": "Chao Peng, Jingbo Wang, Changqian Yu, Huanyu Liu, Xu Liu, Zeming Li, Xiangyu Zhang, Gang Yu, Jian Sun. Megvii (Face++)",
      "name": "Megvii (Face++)"
    },
    "results": "{\"PQ\": 0.532, \"SQ\": 0.832, \"RQ\": 0.629, \"PQTH\": 0.622, \"SQTH\": 0.855, \"RQTH\": 0.725, \"PQST\": 0.395, \"SQST\": 0.797, \"RQST\": 0.485}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-07-31",
    "description": "",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 2,
      "members": "",
      "name": "LaoYang"},
    "results": "{\"PQ\": 0.478, \"SQ\": 0.807, \"RQ\": 0.576, \"PQTH\": 0.574, \"SQTH\": 0.837, \"RQTH\": 0.682, \"PQST\": 0.333, \"SQST\": 0.760, \"RQST\": 0.416}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-17",
    "description": "We get panoptic segmentation result by combining stuff result from semantic segmentation and thing result from instance segmentation with the strategy described in Panoptic Segmentation[1]. For segmentation of thing, we develop a region-based object detection and instance segmentation approach, which follows Mask-RCNN[2]. Feature pyramid network[3] is used as our backbone for multi-scale feature extraction. And we use Cascade-RCNN[4] as our box head, which iteratively regress and refine the box location. For mask head, we propose a new module called Cascade-MASK, which exploits the ROIs of different stage from box head to iteratively refine the mask results and improve the mask result by 1.3% on val5000 in our best res50 based model. Finally we use senet154 as backbone to generate the segmentation results of thing. For segmentation of stuff, we follow the FCN way which regard semantic segmentation as pixel-wise classification. We combine all thing classes as one class for training. Feature pyramid network[3] modified with deformable convolution[5] and non-local convolution[6] is used as our backbone. For all submissions, we only use the ImageNet pretrained model and train with coco2017 panoptic segmentation annotations and instance segmentation annotations. submission 1 combined stuff result using resnext152 64g as backbone and thing result using senet as backbone train with instance segmentaion annotation. submission 2 combined stuff result using ensamble model(resnext152 64g and senet 154 and resnext152 32g and resnet 152) as backbone and thing result using senet as backbone train with instance segmentaion annotation. submission 3 combined stuff result using ensamble model(resnext152 64g and senet 154 and resnext 152 32g and resnet 152) as backbone and thing result using senet as backbone train with instance segmentaion annotation. submission 4 combined stuff result using ensamble model(resnext152 64g and senet 154 and resnext 152 32g) as backbone and thing result using senet as backbone train with instance segmentaion annotation. submission 5 combined stuff result using ensamble model(resnext152 64g and senet 154 and resnext 152 32g and resnet 152) as backbone and thing result using senet as backbone train with panoptic segmentaion annotation. Performance for single models on test-dev is listed: resnext152 64g: 0.4555 senet 154: 0.4553 resnext152 32g:0.4544 resnet152: 0.4518 [1] Kirillov A, He K, Girshick R B, et al. Panoptic Segmentation.[J]. arXiv: Computer Vision and Pattern Recognition, 2018. [2] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. international conference on computer vision, 2017: 2980-2988. [3] Lin T, Dollar P, Girshick R B, et al. Feature Pyramid Networks for Object Detection[J]. computer vision and pattern recognition, 2017: 936-944. [4] Cai Z, Vasconcelos N. Cascade R-CNN: Delving Into High Quality Object Detection[J]. computer vision and pattern recognition, 2018. [5] Dai J, Qi H, Xiong Y, et al. Deformable Convolutional Networks[J]. international conference on computer vision, 2017: 764-773. [6] Wang X, Girshick R B, Gupta A, et al. Non-Local Neural Networks[J]. computer vision and pattern recognition, 2018.",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 3,
      "members": "Yanwei Li*[a], Naiyu Gao*[b], Chaoxu Guo[a], Xinze Chen[a], Qian Zhang[a], Guan Huang[a], Xin Zhao[b], Kaiqi Huang[b], Dalong Du[a], Chang Huang[a](* indicates equal contribution); [a] Horizon Robotics, [b] CRISE, CASIA",
      "name": "Caribbean"},
    "results": "{\"PQ\": 0.468, \"SQ\": 0.805, \"RQ\": 0.571, \"PQTH\": 0.543, \"SQTH\": 0.818, \"RQTH\": 0.659, \"PQST\": 0.355, \"SQST\": 0.785, \"RQST\": 0.438}",
    "results_details": ""
  },
  {"leaderboard_name": "panoptic-dev",
    "date": "2018-08-18",
    "description": "We get instance segmentation and semantic segmentation results separately. Our instance segmentation model is based on ResNeXt-152-FPN structure, and is improved by non-local module, SE module, and bottom-up path aggregation in an alternate updating manner. Our single instance segmentation model gets 43.5 mAP on test-dev. For semantic segmentation, we train a ResNeXt-152-FPN with label bank. A Conv-LSTM model is trained to refine the instance segmentation results. We use thing segments to override stuff segments to generate the panoptic output. Label prior and some thresholds are used to improve the final performance. For our submitted entry, we use the average of two semantic segmentation output. No ensemble is made on panoptic output. No external data is used.",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 4,
      "members": "Yibo Yang, Xia Li, Hongyang Li, Tiancheng Shen, Zhouchen Lin, Jian Dong, Jiashi Feng, Shuicheng Yan",
      "name": "PKU_360"},
    "results": "{\"PQ\": 0.463, \"SQ\": 0.796, \"RQ\": 0.561, \"PQTH\": 0.586, \"SQTH\": 0.837, \"RQTH\": 0.696, \"PQST\": 0.276, \"SQST\": 0.736, \"RQST\": 0.356}",
    "results_details": ""},
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-18",
    "description": "simple combine of semantic from PSPNet50 (train for 50 epochs) and instance from Mask-RCNN101 (train for 180k iterations)",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 6,
      "members": "",
      "name": "ps"
    },
    "results": "{\"PQ\": 0.416, \"SQ\": 0.795, \"RQ\": 0.507, \"PQTH\": 0.504, \"SQTH\": 0.826, \"RQTH\": 0.605, \"PQST\": 0.284, \"SQST\": 0.749, \"RQST\": 0.359}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-12-21",
    "description": "Panoptic Feature Pyramid Networks",
    "leaderboard_id": 101,
    "url": "https://arxiv.org/abs/1901.02446",
    "team": {
      "id": 7,
      "members": "Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Doll√°r",
      "name": "PanopticFPN"
    },
    "results": "{\"PQ\": 0.409, \"SQ\": 0.785, \"RQ\": 0.501, \"PQTH\": 0.483, \"SQTH\": 0.817, \"RQTH\": 0.583, \"PQST\": 0.297, \"SQST\": 0.737, \"RQST\": 0.373}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-18",
    "description": "we used an ensemble model for the COCO 2018 Panoptic Segmentation Challenge. For the instance segmentation task, we fine tuned a pretrained Mask-RCNN [1] network with the COCO panoptic annotations while we trained from scratch a DeepLabv3+ [2] model for the semantic segmentation task. The two-channel output is created by combining the output of the said models, prioritizing the instance masks over the semantic masks. The panoptic quality of the final output reached 36.0 on the COCO test-dev2017 subset.",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 8,
      "members": "Darwin Bautista, Jose Marie Mendoza, Rowel Atienza. Electrical and Electronics Engineering Institute. University of the Philippines.",
      "name": "TeamPH"
    },
    "results": "{\"PQ\": 0.359, \"SQ\": 0.767, \"RQ\": 0.449, \"PQTH\": 0.441, \"SQTH\": 0.800, \"RQTH\": 0.545, \"PQST\": 0.236, \"SQST\": 0.716, \"RQST\": 0.303}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-08-27",
    "description": "",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 9,
      "members": "",
      "name": "sagieppel"},
    "results": "{\"PQ\": 0.337, \"SQ\": 0.796, \"RQ\": 0.414, \"PQTH\": 0.351, \"SQTH\": 0.804, \"RQTH\": 0.429, \"PQST\": 0.315, \"SQST\": 0.784, \"RQST\": 0.393}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-19",
    "description": "Our method is based on Mask-rcnn[1]. And only the COCO panoptic data (no external data) is used for training our method. For submission 1:The DCN[2] and Direction feature[3] are added to the Mask-rcnn framework. For submission 2:The DCN[2] and Direction feature[3] are added to the Mask-rcnn framework. And we train with the stuff and things annotations separately. For submission 3:The DCN[2] and Direction feature[3] are added to the Mask-rcnn framework. Then we consider the objects segmentations as layered set which some objects(like small things) are on top of other objects(like stuff). For submission 4:This submission on Codalab evaluation server is failed. For submission 5:The DCN[2] and Direction feature[3] are added to the Mask-rcnn framework. Then we consider the objects segmentations as layered set which some objects(like small things) are on top of other objects(like stuff). Lastly, we fill up some blank pixels with sutff by Nearest Neighbor. For submission 6:The DCN[2] and Direction feature[3] are added to the Mask-rcnn framework. Then we consider the objects segmentations as layered set which some objects(like small things) are on top of other objects(like stuff). Lastly, we fill up all blank pixels by Nearest Neighbor. [1] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017, PP(99):1-1. [2] Dai J, Qi H, Xiong Y, et al. Deformable convolutional networks[J]. CoRR, abs/1703.06211, 2017, 1(2): 3. [3] Chen L C, Hermans A, Papandreou G, et al. MaskLab: Instance segmentation by refining object detection with semantic and direction features[J]. arXiv preprint arXiv:1712.04837, 2017.",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 10,
      "members": "Qi Zheng, Min Yang, Chenyang Li, Peng Huang, Chao Liang, Jun Chen",
      "name": "MMAP-seg"
    },
    "results": "{\"PQ\": 0.322, \"SQ\": 0.760, \"RQ\": 0.408, \"PQTH\": 0.390, \"SQTH\": 0.782, \"RQTH\": 0.491, \"PQST\": 0.220, \"SQST\": 0.728, \"RQST\": 0.284}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-16",
    "description": "The method makes instance segmentation and semantic segmentation predictions in a single network, and combines these outputs using heuristics to create a single panoptic segmentation output. The architecture consists of a ResNet-50 feature extractor shared by the semantic segmentation and instance segmentation branch. For instance segmentation, a Mask R-CNN type of architecture is used, while the semantic segmentation branch is augmented with a Pyramid Pooling Module. The feature extractor is initialized using weights from a model that is pre-trained on ImageNet. The network is trained on a single Titan Xp GPU, and the submitted results are from a single model.",
    "leaderboard_id": 101,
    "url": "https://arxiv.org/abs/1809.02110",
    "team": {
      "id": 11,
      "members": "Daan de Geus, Panagiotis Meletis, Gijs Dubbelman. Eindhoven University of Technology.",
      "name": "MPS-TU Eindhoven"
    },
    "results": "{\"PQ\": 0.272, \"SQ\": 0.719, \"RQ\": 0.359, \"PQTH\": 0.296, \"SQTH\": 0.716, \"RQTH\": 0.394, \"PQST\": 0.234, \"SQST\": 0.723, \"RQST\": 0.306}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-17",
    "description": "Based on the powerful FPN-Mask-RCNN [1,2] and Deeplabv3+ [3] model, we explore the possibility of combining the two models into a single e2e trainable model. When comparing the two models side by side, we can observe that both models share a similar \u201cencoder-decoder\u201d architecture that can be combined. In FPN-Mask-RCNN, the FPN backbone [1], which has an \u201cencoder-decoder\u201d architecture, can be used to create the Deeplabv3+ model. This is done by adding a FCN-style semantic prediction head onto the final feature-map of the pyramid structure. We reason that since each pyramid level contains information from the previous layer and also from the encoder branch through the skip connections, the final feature-map should contain the relevant information needed for pixel-wise semantic segmentation. In addition to the semantic head, we also incorporate the ASPP-layer from Deeplabv3+ into the FPN backbone. The model is then trained e2e (stably) using a combination of 6 losses: 5 losses from the standard Mask-RCNN and 1 from the semantic head. For the final panoptic segmentation, we employ only an adaptive thresholding method (AT-Fusion) to fuse the output predictions from the Mask-RCNN head and also the semantic prediction head. The fusion is done by first fusing the instances together using adaptive thresholding and prediction scores, while the semantic prediction is used to fill in the background. The adaptive thresholding is done by coupling the final thresholding of the instance detection with the prediction probability of the semantic head. There are 2 reasons for this adaptive thresholding. Firstly, compared to just using a certain region in the Mask-RCNN head, the semantic head uses the information from the entire image/scene for prediction, making the prediction much more reliable. Secondly, as \u201cargmax\u201d is used to produce the final semantic labels, a certain label doesn\u2019t need to have a 0.99 prediction probability to be chosen as the final output. This means that if semantic prediction can be performed without hard constraint, then the same idea can also be applied to the instance detection. As of the time of submission, the first results are submitted for the challenge while experiments and works are still being done to validate the performance of the proposed method. [1] LIN, Tsung-Yi, et al. Feature Pyramid Networks for Object Detection. In: CVPR. 2017. S. 4. [2] HE, Kaiming, et al. Mask r-cnn. In: Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017. S. 2980-2988. [3] CHEN, Liang-Chieh, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation. arXiv preprint arXiv:1802.02611, 2018.",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 12,
      "members": "Ee Heng Chen",
      "name": "LeChen"
    },
    "results": "{\"PQ\": 0.263, \"SQ\": 0.742, \"RQ\": 0.332, \"PQTH\": 0.313, \"SQTH\": 0.762, \"RQTH\": 0.393, \"PQST\": 0.187, \"SQST\": 0.712, \"RQST\": 0.241}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-17",
    "description": "Dense32layer based FCN all in one Panoptic Segementation.all in one model(==single network inference) runs 512x512 image size (~90ms@TitanXp, ~130ms@TitanX pytorch0.4) external data : ImageNet (base pretrain), Cityscapes (base semseg trained), Mapillary v1.0 (base semseg trained. not v1.1 since we received v1.1 on 8/4) Semseg downstream is up to 1/16, uses 1/4 and 1/16 feature for instance masks, alike MaskRCNN but is based on modified RetinaNet with 4xConvBNs for class/box instance mask is generated only 14x14 but later to be warped and fused with SemSeg result. Mask uses class wise mask HxWxC (not class agnostic)",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 13,
      "members": "Tamaki Kojima",
      "name": "Artemis"
    },
    "results": "{\"PQ\": 0.168, \"SQ\": 0.716, \"RQ\": 0.220, \"PQTH\": 0.168, \"SQTH\": 0.724, \"RQTH\": 0.220, \"PQST\": 0.167, \"SQST\": 0.704, \"RQST\": 0.219}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-19",
    "description": "",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 14,
      "members": "",
      "name": "grasshopyx"
    },
    "results": "{\"PQ\": 0.026, \"SQ\": 0.284, \"RQ\": 0.034, \"PQTH\": 0.000, \"SQTH\": 0.000, \"RQTH\": 0.000, \"PQST\": 0.066, \"SQST\": 0.713, \"RQST\": 0.084}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2018-08-07",
    "description": "",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 15,
      "members": "",
      "name": "microljy"
    },
    "results": "{\"PQ\": 0.021, \"SQ\": 0.312, \"RQ\": 0.026, \"PQTH\": 0.033, \"SQTH\": 0.410, \"RQTH\": 0.041, \"PQST\": 0.003, \"SQST\": 0.165, \"RQST\": 0.004}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/megvii_coco2019Panoptic-TechReport.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 16,
      "members": "Shen Wang(Peking University), Tao Liu(Beijing Normal University), Huanyu Liu(Megvii Inc), Yuchen Ma(Megvii Inc), Zeming Li(Megvii Inc), Zhicheng Wang(Megvii Inc), Xinyu Zhou(Megvii Inc), Gang Yu(Megvii Inc), Erjin Zhou(Megvii Inc), Xiangyu Zhang(Megvii Inc), Jian Sun(Megvii Inc)",
      "name": "Megvii"
    },
    "results": "{\"PQ\": 0.547, \"SQ\": 0.836, \"RQ\": 0.643, \"PQTH\": 0.646, \"SQTH\": 0.862, \"RQTH\": 0.746, \"PQST\": 0.398, \"SQST\": 0.797, \"RQST\": 0.488}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/Innovation_coco_report_latest_version.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 17,
      "members": "Chongsong Chen(1), Jiawei Ren(1), Daisheng Jin(2), Zhongang Cai(1), Cunjun Yu(1), Bairun Wang(2), Mingyuan Zhang(2), Jinyi Wu(3). Affiliations: (1): Nanyang Technological University, (2): Beihang University, (3): National University of Singapore",
      "name": "Innovation"
    },
    "results": "{\"PQ\": 0.535, \"SQ\": 0.833, \"RQ\": 0.633, \"PQTH\": 0.618, \"SQTH\": 0.849, \"RQTH\": 0.724, \"PQST\": 0.410, \"SQST\": 0.809, \"RQST\": 0.496}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-03",
    "description": "See <a href=\"../files/panoptic_2019_reports/zigzag_panoptic.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 18,
      "members": "Dingguo Shen, Siting Shen, Yuanfeng Ji, Di Lin. VCC (Shenzhen University)",
      "name": "VCC"
    },
    "results": "{\"PQ\": 0.502, \"SQ\": 0.819, \"RQ\": 0.603, \"PQTH\": 0.583, \"SQTH\": 0.838, \"RQTH\": 0.692, \"PQST\": 0.380, \"SQST\": 0.790, \"RQST\": 0.470}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/COCO_2019_Report_AntVision.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 19,
      "members": "Weixiang Hong, Qingpei Guo, Wei Zhang, Jingdong Chen, Wei Chu (all Ant Financial Services Group)",
      "name": "AntVision"
    },
    "results": "{\"PQ\": 0.500, \"SQ\": 0.821, \"RQ\": 0.598, \"PQTH\": 0.581, \"SQTH\": 0.839, \"RQTH\": 0.686, \"PQST\": 0.378, \"SQST\": 0.792, \"RQST\": 0.465}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-03",
    "description": "See <a href=\"../files/panoptic_2019_reports/tech_report_AiRiA-Panoptic.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 20,
      "members": "Qiang Chen[1,2], Anda Cheng[1,2], Weihan Chen[1,2], Peisong Wang[1], Xiangyu He[1,2], Qinghao Hu[1], Cong Leng[1,3], Jian Cheng[1,2,3]; [1] National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; [2] University of Chinese Academy  of Sciences; [3] AiRiA",
      "name": "AiRiA-Panoptic"
    },
    "results": "{\"PQ\": 0.502, \"SQ\": 0.817, \"RQ\": 0.599, \"PQTH\": 0.574, \"SQTH\": 0.838, \"RQTH\": 0.680, \"PQST\": 0.393, \"SQST\": 0.787, \"RQST\": 0.478}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/PKU_ZERO.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 21,
      "members": "Yibo Yang (Peking Univ.), Xia Li (Peking Univ.), HongYang Li (Peking Univ.), Tianchegn Shen (Peking Univ.), Yudong Liu (Peking Univ.), Zhouchen Lin (Peking Univ.)",
      "name": "PKU_ZERO"
    },
    "results": "{\"PQ\": 0.500, \"SQ\": 0.818, \"RQ\": 0.600, \"PQTH\": 0.592, \"SQTH\": 0.836, \"RQTH\": 0.703, \"PQST\": 0.361, \"SQST\": 0.790, \"RQST\": 0.443}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/eleme_panoptic_segmentation_report.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 22,
      "members": "Shuchun Liu, Feiyun Zhang, Li Long, Weiyuan Shao, Jiajun Wang (all The AI Lab of ELEME Inc)",
      "name": "Eleme"
    },
    "results": "{\"PQ\": 0.470, \"SQ\": 0.814, \"RQ\": 0.564, \"PQTH\": 0.572, \"SQTH\": 0.839, \"RQTH\": 0.676, \"PQST\": 0.315, \"SQST\": 0.775, \"RQST\": 0.394}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/eyecool_tech_report.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 23,
      "members": "Qi Wang, Hailong Zhang, Shi Ma, Yuanshuai Wang, Mei Yang, Jing Li, Feng Li, Xiangde Zhang (College of Sciences, Northeastern University), Wuming Jiang (Beijing Eyecool Technology Co., Ltd)",
      "name": "Eyecool"
    },
    "results": "{\"PQ\": 0.471, \"SQ\": 0.810, \"RQ\": 0.572, \"PQTH\": 0.538, \"SQTH\": 0.822, \"RQTH\": 0.647, \"PQST\": 0.372, \"SQST\": 0.793, \"RQST\": 0.458}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/Tech_report_2019_panoptic_segmentation_SiemensMobility.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 24,
      "members": "Mehmet Yildirim, Yogesh Langhe, Jan Richter, Nanzhu Jiang, Yaroslav Tarasenko, Lyubov Klein, Claus Bahlmann (all Siemens Mobility)",
      "name": "mobility.ai"
    },
    "results": "{\"PQ\": 0.470, \"SQ\": 0.800, \"RQ\": 0.563, \"PQTH\": 0.553, \"SQTH\": 0.841, \"RQTH\": 0.653, \"PQST\": 0.345, \"SQST\": 0.738, \"RQST\": 0.429}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-05",
    "description": "See <a href=\"../files/panoptic_2019_reports/PCV_pixel_consensus_voting.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 25,
      "members": "Haochen Wang (CMU), Ruotian Luo (TTI-Chicago), Greg Shakhnarovich(TTI-Chicago)",
      "name": "PCV"
    },
    "results": "{\"PQ\": 0.432, \"SQ\": 0.799, \"RQ\": 0.529, \"PQTH\": 0.485, \"SQTH\": 0.815, \"RQTH\": 0.588, \"PQST\": 0.351, \"SQST\": 0.774, \"RQST\": 0.442}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-08-27",
    "description": "See <a href=\"../files/panoptic_2019_reports/sagieppel_tech_report_PDF.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 26,
      "members": "Sagi Eppel, Alan Aspuru-Guzik (both University of Toronto)",
      "name": "The Matter Lab"
    },
    "results": "{\"PQ\": 0.337, \"SQ\": 0.796, \"RQ\": 0.414, \"PQTH\": 0.351, \"SQTH\": 0.804, \"RQTH\": 0.429, \"PQST\": 0.315, \"SQST\": 0.784, \"RQST\": 0.393}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-03",
    "description": "See <a href=\"../files/panoptic_2019_reports/marweb_weber-iccv_coco.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
      "id": 27,
      "members": "Mark Weber, Jonathon Luiten and Bastian Leibe (all RWTH Aachen University)",
      "name": "RWTHVision"
    },
    "results": "{\"PQ\": 0.326, \"SQ\": 0.743, \"RQ\": 0.420, \"PQTH\": 0.350, \"SQTH\": 0.748, \"RQTH\": 0.448, \"PQST\": 0.290, \"SQST\": 0.736, \"RQST\": 0.377}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2019-10-04",
    "description": "See <a href=\"../files/panoptic_2019_reports/FPSNet_Tech_Report_COCO_2019.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "https://arxiv.org/abs/1910.03892",
    "team": {
      "id": 28,
      "members": "Daan de Geus (Eindhoven University of Technology), Panagiotis Meletis (Eindhoven University of Technology), Gijs Dubbelman (Eindhoven University of Technology)",
      "name": "FPSNet"
    },
    "results": "{\"PQ\": 0.298, \"SQ\": 0.743, \"RQ\": 0.379, \"PQTH\": 0.331, \"SQTH\": 0.761, \"RQTH\": 0.416, \"PQST\": 0.249, \"SQST\": 0.715, \"RQST\": 0.324}",
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2020-08-23",
    "description": "See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/panoptic/AIC_OpenVision.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
        "id": 29,
        "members": "Biwen Lei (Alibaba Group and Zhejiang University), Miaomiao Cui (Alibaba Group), Jianqiang Ren (Alibaba Group), and Shang Liu (Alibaba Group)",
        "name": "AIC-OpenVisio"
    },
    "results": "{\"PQ\": 0.504, \"SQ\": 0.823, \"RQ\": 0.601, \"PQTH\": 0.581, \"SQTH\": 0.844, \"RQTH\": 0.683, \"PQST\": 0.388, \"SQST\": 0.791, \"RQST\": 0.476}", 
    "results_details": ""
  },
  {
    "leaderboard_name": "panoptic-dev",
    "date": "2020-08-23",
    "description": "See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/panoptic/ELEME.pdf\">technical report</a>",
    "leaderboard_id": 101,
    "url": "",
    "team": {
        "id": 30,
        "members": "Shuchun Liu, Haonan Qiu, Weiyuan Shao, Feiyun Zhang, Yaowu Wei, Peiyao Zhang, and Jiajun Wang (all the AI Lab of ELEME Inc, China)",
        "name": "ELEME"
    },
    "results": "{\"PQ\": 0.499, \"SQ\": 0.812, \"RQ\": 0.596, \"PQTH\": 0.620, \"SQTH\": 0.844, \"RQTH\": 0.730, \"PQST\": 0.315, \"SQST\": 0.763, \"RQST\": 0.394}", 
    "results_details": ""
  }
]
