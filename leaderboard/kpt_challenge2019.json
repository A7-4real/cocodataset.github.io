[{"leaderboard_id": 25, "url": "https://ilovepose.github.io/coco/", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1271, "members": "Hanbin Dai[1,*] Liangbo Zhou[1,*] Feng Zhang[1,*] Zhengyu Zhang[2,*] Hong Hu[1,*]  Xiatian Zhu[3,*] Mao Ye [1] 1: University of Electronic Science and Technology of China; 2: Shenzhen University; 3: University of Surrey; * means equal contribution.", "name": "DarkPose"}, "results": "{\"AP\": \"0.764\", \"AP_50\": \"0.925\", \"AP_75\": \"0.827\", \"AP_medium\": \"0.709\", \"AP_large\": \"0.838\", \"AR\": \"0.816\", \"AR_50\": \"0.957\", \"AR_75\": \"0.875\", \"AR_medium\": \"0.765\", \"AR_large\": \"0.885\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "We adopt a two-stage top-down pipeline: first detect human bounding boxes using HRNetV2p-W48 with Hybrid Task Cascade architecture (59.6 AP for the person category on the COCO validation set) and then perform single person pose estimation on the detected bounding boxes. The pose estimator is our own extended version of HRNet, which has SE-residual unit, skip-connection across modules and modified head architecture. We first pretrain the model with MSE over keypoint heatmaps and then fine-tune the model using L1 loss for regressed keypoint positions obtained by soft-argmax operation on the heatmaps to improve localization accuracy. AI Challenger dataset is used for pretraining. Our single model achieves 75.8 AP on the test-dev set. Our final ensemble results averaging predicted coordinates by the pretrained heatmap model and the fine-tuned regression model achieves 75.9 AP on the test-dev set and 73.1 AP on the test-challenge set.", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1272, "members": "Naoki Kato, Hideki Okada, Yusuke Uchida (all members are belonging to DeNA Co., Ltd.)", "name": "Dproject"}, "results": "{\"AP\": \"0.731\", \"AP_50\": \"0.901\", \"AP_75\": \"0.792\", \"AP_medium\": \"0.682\", \"AP_large\": \"0.812\", \"AR\": \"0.792\", \"AR_50\": \"0.946\", \"AR_75\": \"0.850\", \"AR_medium\": \"0.739\", \"AR_large\": \"0.864\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "We use a two stage top-down method. In the first stage, we use Hybrid Task Cascade to detect human bounding boxes. In the second stage, we use our Structured Fusion Network with Deconvolution Head and two-stage network to detect human keypoints. Finally we use Refinement Network to refine the results got by using ensemble method(5 models). We train models using extra data (AI-Challenger dataset) . Our best single model has 0.764 of mAP on COCO test-dev dataset, and final result has 0.775 of mAP on COCO test-dev dataset and 0.751 of mAP on COCO test-challenge dataset.", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1273, "members": "Jia Wei Netease Game AI Lab weijia@corp.netease.com, Yanjun Li Netease Game AI Lab liyanjun@corp.netease.com", "name": "NeteaseGameAI"}, "results": "{\"AP\": \"0.751\", \"AP_50\": \"0.912\", \"AP_75\": \"0.812\", \"AP_medium\": \"0.704\", \"AP_large\": \"0.824\", \"AR\": \"0.805\", \"AR_50\": \"0.949\", \"AR_75\": \"0.861\", \"AR_medium\": \"0.753\", \"AR_large\": \"0.874\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1274, "members": "Yuanhao Cai*, Zhicheng Wang*, Binyi Yin, Ruihao Yin, Angang Du, Zhengxiong Luo, Zeming Li, Xinyu Zhou, Gang Yu, Erjin Zhou, Xiangyu Zhang, Yichen Wei, Jian Sun (* indicates equal contribution) Affiliations: Megvii Research", "name": "Megvii"}, "results": "{\"AP\": \"0.771\", \"AP_50\": \"0.933\", \"AP_75\": \"0.836\", \"AP_medium\": \"0.722\", \"AP_large\": \"0.836\", \"AR\": \"0.826\", \"AR_50\": \"0.961\", \"AR_75\": \"0.882\", \"AR_medium\": \"0.780\", \"AR_large\": \"0.887\"}", "results_details": {"ensemble": true, "external_data": false}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "We built a single-stage system, which estimates bounding boxes and corresponding keypoints in a single model. Based on Mask R-CNN[1], we develop a novel ROI feature extraction method using channel attention. Combined with multi-scale keypoint head network[2], our model achieve 57.2AP for human detection and 70.3AP for keypoint on COCO 2017 validation set with ResNet50-FPN backbone. No external data is used for training. We did not use model ensemble strategy. [1] Mask R-CNN, Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick, ICCV 2017 [2] Multi-scale Aggregation R-CNN for 2D Multi-person Pose Estimation, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee, CVPRW 2019", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1275, "members": "Myunggu Kang, Clova AI, NAVER Corp (myunggu.kang@navercorp.com) Dongyoon Wee, Clova AI, NAVER Corp (dongyoon.wee@navercorp.com)", "name": "MscaRCNN"}, "results": "{\"AP\": \"0.690\", \"AP_50\": \"0.887\", \"AP_75\": \"0.755\", \"AP_medium\": \"0.641\", \"AP_large\": \"0.772\", \"AR\": \"0.764\", \"AR_50\": \"0.938\", \"AR_75\": \"0.822\", \"AR_medium\": \"0.713\", \"AR_large\": \"0.833\"}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1276, "members": "Dongdong Yu[1,*], Kai Su[1,*], Changhu Wang[1]; 1: ByteDance AI Lab, * means Equal Contributions.", "name": "ByteDanceVC"}, "results": "{\"AP\": \"0.763\", \"AP_50\": \"0.921\", \"AP_75\": \"0.824\", \"AP_medium\": \"0.714\", \"AP_large\": \"0.841\", \"AR\": \"0.823\", \"AR_50\": \"0.957\", \"AR_75\": \"0.877\", \"AR_medium\": \"0.771\", \"AR_large\": \"0.892\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_id": 25, "url": "https://github.com/vita-epfl/openpifpaf", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1277, "members": "Sven Kreiss, Alexandre Alahi, EPFL", "name": "PifPaf"}, "results": "{\"AP\": \"0.694\", \"AP_50\": \"0.876\", \"AP_75\": \"0.758\", \"AP_medium\": \"0.647\", \"AP_large\": \"0.765\", \"AR\": \"0.760\", \"AR_50\": \"0.917\", \"AR_75\": \"0.815\", \"AR_medium\": \"0.704\", \"AR_large\": \"0.836\"}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1278, "members": "Sanghoon Hong, Kakao Brain, sanghoon.hong@kakaobrain.com, Hunchul Park, Kakao Brain, robert.p@kakaobrain.com, Jonghyuk Park, Seoul National University, chico2121@snu.ac.kr, Sukhyun Cho, Seoul National University, chosh90@snu.ac.kr, Heewoong Park, Seoul National University, hee188@snu.ac.kr", "name": "KakaoBrain"}, "results": "{\"AP\": \"0.753\", \"AP_50\": \"0.919\", \"AP_75\": \"0.817\", \"AP_medium\": \"0.699\", \"AP_large\": \"0.828\", \"AR\": \"0.806\", \"AR_50\": \"0.950\", \"AR_75\": \"0.864\", \"AR_medium\": \"0.755\", \"AR_large\": \"0.875\"}", "results_details": {"ensemble": true, "external_data": false}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1279, "members": "Jing Zhang, Zhe Chen and Dacheng Tao, UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia", "name": "Usaic"}, "results": "{\"AP\": \"0.755\", \"AP_50\": \"0.923\", \"AP_75\": \"0.821\", \"AP_medium\": \"0.699\", \"AP_large\": \"0.828\", \"AR\": \"0.811\", \"AR_50\": \"0.956\", \"AR_75\": \"0.871\", \"AR_medium\": \"0.762\", \"AR_large\": \"0.879\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1280, "members": "Shuchun Liu, Feiyun Zhang, Li Long, Weiyuan Shao, Jiajun Wang", "name": "Eleme"}, "results": "{\"AP\": \"0.744\", \"AP_50\": \"0.917\", \"AP_75\": \"0.806\", \"AP_medium\": \"0.692\", \"AP_large\": \"0.822\", \"AR\": \"0.799\", \"AR_50\": \"0.951\", \"AR_75\": \"0.856\", \"AR_medium\": \"0.746\", \"AR_large\": \"0.871\"}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_id": 25, "url": "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1281, "members": "Bin Xiao (ByteDance), Zaizhou Gong (ByteDance), Yifan Lu (ByteDance), Linfu Wen (ByteDance)", "name": "ByteDanceHRNet"}, "results": "{\"AP\": \"0.755\", \"AP_50\": \"0.914\", \"AP_75\": \"0.815\", \"AP_medium\": \"0.704\", \"AP_large\": \"0.838\", \"AR\": \"0.813\", \"AR_50\": \"0.952\", \"AR_75\": \"0.870\", \"AR_medium\": \"0.761\", \"AR_large\": \"0.884\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": "https://arxiv.org/abs/1902.09212, https://arxiv.org/abs/1908.07919"}, {"leaderboard_id": 25, "url": "https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1282, "members": "Bowen Cheng (UIUC), Bin Xiao (ByteDance), Jingdong Wang (Microsoft), Honghui Shi (UIUC, University of Oregon), Thomas S. Huang (UIUC), Lei Zhang (Microsoft)", "name": "HigherHRNet"}, "results": "{\"AP\": \"0.700\", \"AP_50\": \"0.878\", \"AP_75\": \"0.761\", \"AP_medium\": \"0.648\", \"AP_large\": \"0.773\", \"AR\": \"0.752\", \"AR_50\": \"0.903\", \"AR_75\": \"0.804\", \"AR_medium\": \"0.692\", \"AR_large\": \"0.834\"}", "results_details": {"ensemble": false, "external_data": false}, "publication": "https://arxiv.org/abs/1908.10357"}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1283, "members": "Zixuan Huang huangzixuan0508@bupt.edu.cn, Yuan Chang changyuan@bupt.edu.cn", "name": "EditC"}, "results": "{\"AP\": \"0.735\", \"AP_50\": \"0.912\", \"AP_75\": \"0.798\", \"AP_medium\": \"0.683\", \"AP_large\": \"0.814\", \"AR\": \"0.791\", \"AR_50\": \"0.949\", \"AR_75\": \"0.850\", \"AR_medium\": \"0.739\", \"AR_large\": \"0.862\"}", "results_details": {"ensemble": false, "external_data": false}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1284, "members": "Junjie Huang (Institute of Automation, Chinese Academy of Sciences); Zheng Zhu (Institute of Automation, Chinese Academy of Sciences); Guan Huang (Institute of Automation, Chinese Academy of Sciences)", "name": "Casia"}, "results": "{\"AP\": \"0.746\", \"AP_50\": \"0.911\", \"AP_75\": \"0.805\", \"AP_medium\": \"0.696\", \"AP_large\": \"0.825\", \"AR\": \"0.808\", \"AR_50\": \"0.950\", \"AR_75\": \"0.861\", \"AR_medium\": \"0.755\", \"AR_large\": \"0.879\"}", "results_details": {"ensemble": true, "external_data": true}, "publication": ""}, {"leaderboard_id": 25, "url": "", "description": "", "leaderboard_name": "kpt-challenge2019", "date": "2019-10-27", "team": {"id": 1285, "members": "Zhang Fangjian, Cheng Wei, Li Liang, Li Liuwu", "name": "Infinova"}, "results": "{\"AP\": \"0.737\", \"AP_50\": \"0.910\", \"AP_75\": \"0.801\", \"AP_medium\": \"0.686\", \"AP_large\": \"0.815\", \"AR\": \"0.793\", \"AR_50\": \"0.948\", \"AR_75\": \"0.852\", \"AR_medium\": \"0.741\", \"AR_large\": \"0.864\"}", "results_details": {"ensemble": false, "external_data": true}, "publication": ""}]