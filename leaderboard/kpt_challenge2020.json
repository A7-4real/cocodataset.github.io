[{"description": " See <a href=\"../files/keypoints_2019_reports/DarkPose.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "https://ilovepose.github.io/coco/", "results": "{\"AP\": \"0.764\", \"AP_50\": \"0.925\", \"AP_75\": \"0.827\", \"AP_medium\": \"0.709\", \"AP_large\": \"0.838\", \"AR\": \"0.816\", \"AR_50\": \"0.957\", \"AR_75\": \"0.875\", \"AR_medium\": \"0.765\", \"AR_large\": \"0.885\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1271, "members": "Hanbin Dai[1,*] Liangbo Zhou[1,*] Feng Zhang[1,*] Zhengyu Zhang[2,*] Hong Hu[1,*]  Xiatian Zhu[3,*] Mao Ye [1] 1: University of Electronic Science and Technology of China; 2: Shenzhen University; 3: University of Surrey; * means equal contribution.", "name": "DarkPose"}, "date": "2019-10-27", "publication": ""}, {"description": "We adopt a two-stage top-down pipeline: first detect human bounding boxes using HRNetV2p-W48 with Hybrid Task Cascade architecture (59.6 AP for the person category on the COCO validation set) and then perform single person pose estimation on the detected bounding boxes. The pose estimator is our own extended version of HRNet, which has SE-residual unit, skip-connection across modules and modified head architecture. We first pretrain the model with MSE over keypoint heatmaps and then fine-tune the model using L1 loss for regressed keypoint positions obtained by soft-argmax operation on the heatmaps to improve localization accuracy. AI Challenger dataset is used for pretraining. Our single model achieves 75.8 AP on the test-dev set. Our final ensemble results averaging predicted coordinates by the pretrained heatmap model and the fine-tuned regression model achieves 75.9 AP on the test-dev set and 73.1 AP on the test-challenge set. See <a href=\"../files/keypoints_2019_reports/Dproject.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.731\", \"AP_50\": \"0.901\", \"AP_75\": \"0.792\", \"AP_medium\": \"0.682\", \"AP_large\": \"0.812\", \"AR\": \"0.792\", \"AR_50\": \"0.946\", \"AR_75\": \"0.850\", \"AR_medium\": \"0.739\", \"AR_large\": \"0.864\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1272, "members": "Naoki Kato, Hideki Okada, Yusuke Uchida (all members are belonging to DeNA Co., Ltd.)", "name": "Dproject"}, "date": "2019-10-27", "publication": ""}, {"description": "We use a two stage top-down method. In the first stage, we use Hybrid Task Cascade to detect human bounding boxes. In the second stage, we use our Structured Fusion Network with Deconvolution Head and two-stage network to detect human keypoints. Finally we use Refinement Network to refine the results got by using ensemble method(5 models). We train models using extra data (AI-Challenger dataset) . Our best single model has 0.764 of mAP on COCO test-dev dataset, and final result has 0.775 of mAP on COCO test-dev dataset and 0.751 of mAP on COCO test-challenge dataset. See <a href=\"../files/keypoints_2019_reports/NeteaseGameAI.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.751\", \"AP_50\": \"0.912\", \"AP_75\": \"0.812\", \"AP_medium\": \"0.704\", \"AP_large\": \"0.824\", \"AR\": \"0.805\", \"AR_50\": \"0.949\", \"AR_75\": \"0.861\", \"AR_medium\": \"0.753\", \"AR_large\": \"0.874\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1273, "members": "Jia Wei Netease Game AI Lab weijia@corp.netease.com, Yanjun Li Netease Game AI Lab liyanjun@corp.netease.com", "name": "NeteaseGameAI"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Megvii.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.771\", \"AP_50\": \"0.933\", \"AP_75\": \"0.836\", \"AP_medium\": \"0.722\", \"AP_large\": \"0.836\", \"AR\": \"0.826\", \"AR_50\": \"0.961\", \"AR_75\": \"0.882\", \"AR_medium\": \"0.780\", \"AR_large\": \"0.887\"}", "results_details": {"ensemble": true, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1274, "members": "Yuanhao Cai*, Zhicheng Wang*, Binyi Yin, Ruihao Yin, Angang Du, Zhengxiong Luo, Zeming Li, Xinyu Zhou, Gang Yu, Erjin Zhou, Xiangyu Zhang, Yichen Wei, Jian Sun (* indicates equal contribution) Affiliations: Megvii Research", "name": "Megvii"}, "date": "2019-10-27", "publication": ""}, {"description": "We built a single-stage system, which estimates bounding boxes and corresponding keypoints in a single model. Based on Mask R-CNN, we develop a novel ROI feature extraction method using channel attention. Combined with multi-scale keypoint head network, our model achieve 57.2AP for human detection and 70.3AP for keypoint on COCO 2017 validation set with ResNet50-FPN backbone. No external data is used for training. We did not use model ensemble strategy. See <a href=\"../files/keypoints_2019_reports/MscaRCNN.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.690\", \"AP_50\": \"0.887\", \"AP_75\": \"0.755\", \"AP_medium\": \"0.641\", \"AP_large\": \"0.772\", \"AR\": \"0.764\", \"AR_50\": \"0.938\", \"AR_75\": \"0.822\", \"AR_medium\": \"0.713\", \"AR_large\": \"0.833\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1275, "members": "Myunggu Kang, Clova AI, NAVER Corp (myunggu.kang@navercorp.com) Dongyoon Wee, Clova AI, NAVER Corp (dongyoon.wee@navercorp.com)", "name": "MscaRCNN"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/ByteDanceVC.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.763\", \"AP_50\": \"0.921\", \"AP_75\": \"0.824\", \"AP_medium\": \"0.714\", \"AP_large\": \"0.841\", \"AR\": \"0.823\", \"AR_50\": \"0.957\", \"AR_75\": \"0.877\", \"AR_medium\": \"0.771\", \"AR_large\": \"0.892\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1276, "members": "Dongdong Yu[1,*], Kai Su[1,*], Changhu Wang[1]; 1: ByteDance AI Lab, * means Equal Contributions.", "name": "ByteDanceVC"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/PifPaf.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "https://github.com/vita-epfl/openpifpaf", "results": "{\"AP\": \"0.694\", \"AP_50\": \"0.876\", \"AP_75\": \"0.758\", \"AP_medium\": \"0.647\", \"AP_large\": \"0.765\", \"AR\": \"0.760\", \"AR_50\": \"0.917\", \"AR_75\": \"0.815\", \"AR_medium\": \"0.704\", \"AR_large\": \"0.836\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1277, "members": "Sven Kreiss, Alexandre Alahi, EPFL", "name": "PifPaf"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/KakaoBrain.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.753\", \"AP_50\": \"0.919\", \"AP_75\": \"0.817\", \"AP_medium\": \"0.699\", \"AP_large\": \"0.828\", \"AR\": \"0.806\", \"AR_50\": \"0.950\", \"AR_75\": \"0.864\", \"AR_medium\": \"0.755\", \"AR_large\": \"0.875\"}", "results_details": {"ensemble": true, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1278, "members": "Sanghoon Hong, Kakao Brain, sanghoon.hong@kakaobrain.com, Hunchul Park, Kakao Brain, robert.p@kakaobrain.com, Jonghyuk Park, Seoul National University, chico2121@snu.ac.kr, Sukhyun Cho, Seoul National University, chosh90@snu.ac.kr, Heewoong Park, Seoul National University, hee188@snu.ac.kr", "name": "KakaoBrain"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Usaic.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.755\", \"AP_50\": \"0.923\", \"AP_75\": \"0.821\", \"AP_medium\": \"0.699\", \"AP_large\": \"0.828\", \"AR\": \"0.811\", \"AR_50\": \"0.956\", \"AR_75\": \"0.871\", \"AR_medium\": \"0.762\", \"AR_large\": \"0.879\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1279, "members": "Jing Zhang, Zhe Chen and Dacheng Tao, UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia", "name": "Usaic"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Eleme.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.744\", \"AP_50\": \"0.917\", \"AP_75\": \"0.806\", \"AP_medium\": \"0.692\", \"AP_large\": \"0.822\", \"AR\": \"0.799\", \"AR_50\": \"0.951\", \"AR_75\": \"0.856\", \"AR_medium\": \"0.746\", \"AR_large\": \"0.871\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1280, "members": "Shuchun Liu, Feiyun Zhang, Li Long, Weiyuan Shao, Jiajun Wang", "name": "Eleme"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/ByteDanceHRNet.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch", "results": "{\"AP\": \"0.755\", \"AP_50\": \"0.914\", \"AP_75\": \"0.815\", \"AP_medium\": \"0.704\", \"AP_large\": \"0.838\", \"AR\": \"0.813\", \"AR_50\": \"0.952\", \"AR_75\": \"0.870\", \"AR_medium\": \"0.761\", \"AR_large\": \"0.884\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1281, "members": "Bin Xiao (ByteDance), Zaizhou Gong (ByteDance), Yifan Lu (ByteDance), Linfu Wen (ByteDance)", "name": "ByteDanceHRNet"}, "date": "2019-10-27", "publication": "https://arxiv.org/abs/1902.09212, https://arxiv.org/abs/1908.07919"}, {"description": " See <a href=\"../files/keypoints_2019_reports/HigherHRNet.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation", "results": "{\"AP\": \"0.700\", \"AP_50\": \"0.878\", \"AP_75\": \"0.761\", \"AP_medium\": \"0.648\", \"AP_large\": \"0.773\", \"AR\": \"0.752\", \"AR_50\": \"0.903\", \"AR_75\": \"0.804\", \"AR_medium\": \"0.692\", \"AR_large\": \"0.834\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1282, "members": "Bowen Cheng (UIUC), Bin Xiao (ByteDance), Jingdong Wang (Microsoft), Honghui Shi (UIUC, University of Oregon), Thomas S. Huang (UIUC), Lei Zhang (Microsoft)", "name": "HigherHRNet"}, "date": "2019-10-27", "publication": "https://arxiv.org/abs/1908.10357"}, {"description": " See <a href=\"../files/keypoints_2019_reports/EditC.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.735\", \"AP_50\": \"0.912\", \"AP_75\": \"0.798\", \"AP_medium\": \"0.683\", \"AP_large\": \"0.814\", \"AR\": \"0.791\", \"AR_50\": \"0.949\", \"AR_75\": \"0.850\", \"AR_medium\": \"0.739\", \"AR_large\": \"0.862\"}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_id": 25, "team": {"id": 1283, "members": "Zixuan Huang huangzixuan0508@bupt.edu.cn, Yuan Chang changyuan@bupt.edu.cn", "name": "EditC"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Casia.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.746\", \"AP_50\": \"0.911\", \"AP_75\": \"0.805\", \"AP_medium\": \"0.696\", \"AP_large\": \"0.825\", \"AR\": \"0.808\", \"AR_50\": \"0.950\", \"AR_75\": \"0.861\", \"AR_medium\": \"0.755\", \"AR_large\": \"0.879\"}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1284, "members": "Junjie Huang (Institute of Automation, Chinese Academy of Sciences); Zheng Zhu (Institute of Automation, Chinese Academy of Sciences); Guan Huang (Institute of Automation, Chinese Academy of Sciences)", "name": "Casia"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"../files/keypoints_2019_reports/Infinova.pdf\">technical report</a>. ", "leaderboard_name": "kpt-challenge2019", "url": "", "results": "{\"AP\": \"0.737\", \"AP_50\": \"0.910\", \"AP_75\": \"0.801\", \"AP_medium\": \"0.686\", \"AP_large\": \"0.815\", \"AR\": \"0.793\", \"AR_50\": \"0.948\", \"AR_75\": \"0.852\", \"AR_medium\": \"0.741\", \"AR_large\": \"0.864\"}", "results_details": {"ensemble": false, "external_data": true}, "leaderboard_id": 25, "team": {"id": 1285, "members": "Zhang Fangjian, Cheng Wei, Li Liang, Li Liuwu", "name": "Infinova"}, "date": "2019-10-27", "publication": ""}, {"description": " See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/keypoints/UDP.pdf\">technical report</a>. ","leaderboard_name": "kpt-challenge2020","url": "https://github.com/HuangJunJie2017/UDP-Pose","results": "{\"AP\": \"0.774\", \"AP_50\": \"0.935\", \"AP_75\": \"0.838\", \"AP_medium\": \"0.718\", \"AP_large\": \"0.847\", \"AR\": \"0.826\", \"AR_50\": \"0.963\", \"AR_75\": \"0.883\", \"AR_medium\": \"0.775\", \"AR_large\": \"0.894\"}","results_details": "{"ensemble": true, "external_data": true}","leaderboard_id": 25,"team": {"id": 1286, "members": "Junjie Huang*, Zengguang Shan*, Yuanhao Cai*, Feng Guo, Yun Ye and Xinze Chen (XForwardAI Technology, * indicates equal contribution), Zheng Zhu (Tsinghua University), Guan Huang (XForwardAI Technology), Jiwen Lu (Tsinghua University), Dalong Du (XForwardAI Technology)", "name": "XForwardAI"},"date": "2020-08-23","publication": ""},{"description": "We use a two stage top-down method. In the first stage, we use DetectoRS to detect human bounding boxes. In the second stage, we use Structured Fusion Network (SFNetv2) to detect human keypoints. Finally we use Refinement Network to refine the results by using ensemble method with 6 models. Our best single model has 0.775 of mAP on COCO test-dev dataset, andfinal result has 0.781 of mAP on COCO test-dev dataset and 0.755 of mAP on COCO test-challenge dataset. See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/keypoints/SFNet.pdf\">technical report</a>.", "leaderboard_name": "kpt-challenge2020", "url": "", "results": "{\"AP\": \"0.755\", \"AP_50\": \"0.917\", \"AP_75\": \"0.817\", \"AP_medium\": \"0.704\", \"AP_large\": \"0.833\", \"AR\": \"0.809\", \"AR_50\": \"0.953\", \"AR_75\": \"0.866\", \"AR_medium\": \"0.758\", \"AR_large\": \"0.877\"}","results_details": "{"ensemble": true, "external_data": true}","leaderboard_id": 25,"team": {"id": 1287, "members": "Li Yanjun; Zhifan Li; Jia Wei", "name": "Netease"},"date": "2020-08-23","publication": ""}, {"description": "This paper is the technical report of team ReferAlgo for COCO Keypoint Challenge 2020. We follow the top-down paradigm in this contest, which firstly applies detectoRS to detect persons from given images, and then estimates keypoints of detected human bodies with HRNet-based methods. Our best single model achieved AP score of 77.8 on COCO test-dev dataset, and the performance of our ensembled methods reached 78.3 on COCO test-dev dataset and 75.9 on COCO test-challenge dataset respectively. See <a href=\"https://s3-us-west-1.amazonaws.com/presentations.cocodataset.org/ECCV20/keypoints/ReferAlgo.pdf\">technical report</a>.",
"leaderboard_name": "kpt-challenge2020","url": "","results": "{\"AP\": \"0.759\", \"AP_50\": \"0.920\", \"AP_75\": \"0.820\", \"AP_medium\": \"0.709\", \"AP_large\": \"0.836\", \"AR\": \"0.811\", \"AR_50\": \"0.953\", \"AR_75\": \"0.867\", \"AR_medium\": \"0.761\", \"AR_large\": \"0.879\"}","results_details": "{"ensemble": true, "external_data": true}","leaderboard_id": 25,"team": {"id": 1288, "members": "Min Du (Horizon Robotics); Kun Zhang (Institute of Computing Technology, Chinese Academy of Sciences); Yuhao Dou (Horizon Robotics); Rui Wu (Horizon Robotics)", "name": "ReferAlgo"},"date": "2020-08-23","publication": ""}]
