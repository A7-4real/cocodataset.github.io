<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">1. Detections Upload</p>
<p>This page describes the <i>upload instructions</i> for submitting results to the detection <a href="https://www.codalab.org/competitions/5181" target="_blank">evaluation server</a>. Before proceeding, please review the <a href="#format">results format</a> and <a href="#detections-eval">evaluation details</a>. Submitting results allows you to participate in the <a href="#detections-challenge2016">COCO Detection Challenge</a> and compare results to the state-of-the-art on the detection <a href="#detections-leaderboard">leaderboard</a>.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">2. Competition Details</p>
<p>The COCO 2015 Test Set can be obtained on the <a href="#download">download page.</a> The recommended training data consists of the COCO 2014 Training and Validation sets. External data of any form is allowed (except of course any form of annotation on the COCO Test set is forbidden). Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.</p>
<p>There are two distinct detection challenges and associated leaderboards: for detectors that output bounding boxes and for detectors that output object segments. The bounding box challenge provides continuity with past challenges such as the PASCAL VOC; the detection by segmentation challenge encourages higher accuracy object localization. The evaluation code in the two cases computes IoU using boxes or segments, respectively, but is otherwise identical. Please see the <a href="#detections-eval">evaluation details</a>.</p>
<p>Please limit the number of entries to the evaluation server to a reasonable number, e.g. one entry per paper. To avoid overfitting, the <i>number of submissions per user is limited to 2 upload per day and a maximum of 5 submissions per user</i>. It is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontSubtitle">2.1. Test Set Splits</p>
<p>The 2015 COCO Test set consists of ~80K test images. To limit overfitting while giving researchers more flexibility to test their system, we have divided the test set into four roughly equally sized splits of ~20K images each: <i>test-dev</i>, <i>test-standard</i>, <i>test-challenge</i>, and <i>test-reserve</i>. Submission to the test set automatically results in submission on each split (identities of the splits are not publicly revealed). In addition, to allow for debugging and validation experiments, we allow researcher <i>unlimited</i> submission to test-dev. Each test split serves a distinct role; details below.</p>
<div class="json fontMono">
  <table class="datasetSplits">
    <tr><th>split</th><th>#imgs</th><th>submission</th><th>scores reported</th></tr>
    <tr><td>Test-Dev</td><td>~20K</td><td>unlimited</td><td>immediately</td></tr>
    <tr><td>Test-Standard</td><td>~20K</td><td>limited</td><td>immediately</td></tr>
    <tr><td>Test-Challenge</td><td>~20K</td><td>limited</td><td>challenge</td></tr>
    <tr><td>Test-Reserve</td><td>~20K</td><td>limited</td><td>never</td></tr>
  </table>
</div>
<p>Test-Dev: We place <i>no limit</i> on the number of submissions allowed to test-dev. In fact, <i>we encourage use of the test-dev for performing validation experiments</i>. Use test-dev to debug and finalize your method before submitting to the full test set.</p>
<p>Test-Standard: The test-standard split is the default test data for the detection competition. <i>When  comparing to the state of the art, results should be reported on test-standard</i>.</p>
<p>Test-Challenge: The test-challenge split is used for the <a href="#detections-challenge2016">COCO Detection Challenge</a>. Results will be revealed during the <a href="http://image-net.org/challenges/ilsvrc+coco2016" target="_blank">ImageNet and COCO Visual Recognition Challenges</a> Workshop.</p>
<p>Test-Reserve: The test-reserve split is used to protect against possible overfitting. If there are substantial differences between a method's scores on test-standard and test-reserve this will raise a red-flag and prompt further investigation. Results on test-reserve will not be publicly revealed.</p>
<p>We emphasize that except for test-dev, results <i>cannot</i> be submitted to a single split and must instead be submitted on the full test set. A submission to the test set populates three leaderboards: test-dev, test-standard and test-challenge (the updated test-challenge leaderboard will not be revealed until the ECCV 2016 Workshop). It is not possible to submit to test-standard without submitting to test-challenge or vice-versa (however, it is possible to submit to the test set without making results public, see  below). The identity of the images in each split is <i>not</i> revealed, except for test-dev.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontSubtitle">2.2. Test-Dev Best Practices</p>
<p>The test-dev 2015 set is a subset of the 2015 Testing set. The specific images belonging to test-dev are listed in the "image_info_test-dev2015.json" file available on the <a href="#download">download page</a> as part of the "2015 Testing Image info" download. As discussed, we place <i>no limit</i> on the number of submissions allowed on test-dev. Note that while submitting to test-dev will produce evaluation results, doing so will not populate the public test-dev leaderboard. Instead, submitting to the full test set populates the test-dev leaderboard. This limits the number of results displayed on the test-dev leaderboard.</p>
<p>Test-dev should be used only for validation and debugging: in a publication <i>it is not acceptable to report results on test-dev only</i>. However, for validation experiments it is acceptable to report results of competing methods on test-dev (obtained from the public test-dev leaderboard). While test-dev is prone to some overfitting, we expect this may still be useful in practice. We emphasize that final comparisons should always be performed on test-standard.</p>
<p>The differences between the validation and test-dev sets are threefold: guaranteed consistent evaluation of test-dev using the evaluation server, test-dev cannot be used for training (annotations are private), and a leaderboard is provided for test-dev, allowing for comparison with the state-of-the-art. We note that the continued popularity of the outdated PASCAL VOC 2007 dataset partially stems from the fact that it allows for simultaneous validation experiments and comparisons to the state-of-the-art. Our goal with test-dev is to provide similar functionality (while keeping annotations private).</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">3. Enter The Competition</p>
<p>First you need to create an account on <a href="https://codalab.org" target="_blank">CodaLab</a>. From your account you will be able to participate in all COCO challenges.</p>
<p>Before uploading your results to the evaluation server, you will need to create a JSON file containing your results in the correct <a href="#format">format</a>. The file should be named "detections_[testset]_[alg]_results.json". Replace [alg] with your algorithm name and [testset] with either "test-dev2015" or "test2015" depending on the test split you are using. Place the JSON file into a zip file named "detections_[testset]_[alg]_results.zip".</p>
<p>To submit your zipped result file to the COCO Detection Challenge click on the “Participate” tab on the CodaLab <a href="https://www.codalab.org/competitions/5181" target="_blank">evaluation server</a>. Select the challenge type (bbox or segm) and test split (test-dev or test). When you select “Submit / View Results” you will be given the option to submit new results. Please fill in the required fields and click “Submit”. A pop-up will prompt you to select the results zip file for upload. After the file is uploaded the evaluation server will begin processing. To view the status of your submission please select “Refresh Status”. Please be patient, the evaluation may take quite some time to complete (~20min on test-dev and ~80min on the full test set). If the status of your submission is “Failed” please check your file is named correctly and has the right <a href="#format">format</a>.</p>
<p>After you submit your results to the evaluation server, you can control whether your results are publicly posted to the CodaLab leaderboard. To toggle the public visibility of your results please select either “post to leaderboard” or “remove from leaderboard”. For now only one result can be published to the leaderboard at any time, we may change this in the future.</p>
<p>In addition to the CodaLab leaderboard, we also host our own more detailed <a href="#detections-leaderboard">leaderboard</a> that includes additional results and method information (such as paper references). Note that the CodaLab leaderboard may contain results not yet migrated to our own leaderboard.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">4. Download Evaluation Results</p>
<p>After evaluation is complete and the server shows a status of “Finished”, you will have the option to download your evaluation results by selecting “Download evaluation output from scoring step.” The zip file will contain three files:</p>
<div class="json fontMono">
  <div class="jsonreg">
    <div class="jsonblk">
      detections_[testset]_[alg]_eval.json<br/>
      metadata<br/>
      scores.txt
    </div>
    <div class="jsonblk">
      % aggregated evaluation on test <br/>
      % auto generated (safe to ignore) <br/>
      % auto generated (safe to ignore)
    </div>
  </div>
</div>
<p>The format of the eval file is described on the <a href="#detections-eval">detection evaluation</a> page.</p>
