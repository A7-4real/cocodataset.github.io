<h1>COCO 2019 Object Detection Task</h1>
<p><img src="images/detection-splash.png" class="wide"/></p>

<h1>1. Overview</h1>
<p>The COCO Object Detection Task is designed to push the state of the art in object detection forward. COCO features two object detection tasks: using either bounding box output or object segmentation output (the latter is also known as instance segmentation). For full details of this task please see the <a href="#detection-eval">detection evaluation</a> page. Note: <b>only the detection task with object segmentation output will be featured at the COCO 2019 challenge</b> (more details follow below).</p>
<p>This task is part of the <a href="workshop/coco-mapillary-iccv-2019.html">Joint COCO and Mapillary Recognition Challenge Workshop</a> at ICCV 2019. For further details about the joint workshop please visit the workshop page. Researchers are encouraged to participate in both the COCO and Mapillary Object Detection Tasks (the tasks share identical data formats and evaluation metrics). Please also see the related COCO <a href="#keypoints-2019">keypoint</a>, <a href="#stuff-2019">stuff</a>, and <a href="#panoptic-2019">panoptic</a> tasks. Whereas the detection task addresses <i>thing</i> classes (person, car, elephant), the <a href="#stuff-2019">stuff</a> task focuses on <i>stuff</i> classes (grass, wall, sky) and the newly introduced <a href="#panoptic-2019">panoptic</a> task addresses both simultaneously.</p>
<p>The COCO train, validation, and test sets, containing more than 200,000 images and 80 object categories, are available on the <a href="#download">download</a> page. All object instances are annotated with a detailed segmentation mask. Annotations on the training and validation sets (with over 500,000 object instances segmented) are publicly available.</p>
<p>This is the fifth iteration of the detection task and it exactly follows the <a href="#detection-2018">COCO 2018 Object Detection Task</a>. In particular, the same data, metrics, and guidelines are being used for this year's task. As in 2018 <i>only the instance segmentation task will be featured at the challenge, with winners being invited to present at the workshop</i>. For detection with bounding boxes outputs, researchers may continue to submit to test-dev, but not to test-challenge, and results will not be presented at the workshop. As detection has steadily advanced, the purpose of this change is to encourage the community to focus on the more challenging and visually informative instance segmentation task.</p>

<h1>2. Dates</h1>
<div class="json">
  <div class="jsonktxt fontBlue">October 4, 2019</div><div class="jsonvtxt">Submission deadline (11:59 PST)</div>
  <div class="jsonktxt">October 11, 2019</div><div class="jsonvtxt">Technical report submission deadline</div>
  <div class="jsonktxt">October 18, 2019</div><div class="jsonvtxt">Challenge winners notified</div>
  <div class="jsonktxt">October 27, 2019</div><div class="jsonvtxt">Winners present at ICCV 2019 Workshop</div>
</div>

<h1>3. New Rules and Awards</h1>
<ul>
  <li>Participants must submit a <strong>technical report</strong> that includes a detailed ablation study of their submission (suggested length 1-4 pages). The reports will be made public. We will provide a latex template for the report. This report will substitute the short text description that we requested previously. Only submissions with the report will be considered for any award and will be put in the COCO leaderboard.</li>
  <li>This year for each challenge track we will have two different awards: <strong>best result award</strong> and <strong>most innovative award</strong>. The most innovative award will be based on the method description in the submitted technical reports and decided by the COCO award committee. The commitee will invite teams to present at the workshop based on the innovations of the submissions rather than the best scores.</li>
  <li>This year we introduce single <strong>best paper award</strong> for the most innovative and successful solution across all challenges. The winner will be determined by the workshop organization committee.</li>
</ul>

<h1>4. Organizers</h1>
<div>TBD</div>

<h1>5. Award Committee</h1>
<div>TBD</div>

<h1>6. Task Guidelines</h1>
<p>Participants are recommended but not restricted to train their algorithms on COCO 2017 train and val sets. The <a href="#download">download</a> page has links to all COCO 2017 data. The COCO test set is divided into two splits: test-dev and test-challenge. Test-dev is as the default test set for testing under general circumstances and is used to maintain a public <a href="#detection-leaderboard">leaderboard</a>. Test-challenge is used for the workshop competition; results will be revealed at the workshop. When participating in this task, please specify any and all external data used for training in the "method description" when uploading results to the evaluation server. A more thorough explanation of all these details is available on the <a href="#guidelines">guidelines</a> page, please be sure to review it carefully prior to participating. Results in the correct <a href="#format-results">format</a> must be <a href="#upload">uploaded</a> to the <a href="https://competitions.codalab.org/competitions/5181" target="_blank">evaluation server</a>. The <a href="#detection-eval">evaluation</a> page lists detailed information regarding how results will be evaluated.</p>

<h1>7. Tools and Instructions</h1>
<p>We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO API, please visit our <a href="https://github.com/cocodataset/cocoapi">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="#download">download</a> page. Due to the large size of COCO and the complexity of this task, the process of participating may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="#download">download</a>, <a href="#format-data">data format</a>, <a href="#format-results">results format</a>, <a href="#guidelines">guidelines</a>, <a href="#upload">upload</a>, and <a href="#detection-eval">evaluation</a> pages. For additional questions, please contact <a href="mailto:info@cocodataset.org">info@cocodataset.org</a>.</p>
