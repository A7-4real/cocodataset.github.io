<h1>COCO 2018 Detection Task</h1>
<p align="center"><img src="images/detections-challenge-splash.png" class="wide"/></p>

<h1>1. Overview</h1>
<p>The COCO 2018 Detection Challenge is designed to push the state of the art in object detection forward. COCO features two object detection tasks: using either bounding box output or object segmentation output (the latter is also known as instance segmentation). For full details of this task please see the <a href="#detections-eval">detection evaluation</a> page. Note: <b>only the detection task with object segmentation output will be featured at the COCO 2018 challenge</b> (more details follow below).</p>
<p>This task is part of the <a href="workshop/coco-mapillary-eccv-2018.html">Joint COCO and Mapillary Recognition Challenge Workshop</a> at ECCV 2018. For further details about the joint workshop please visit the workshop website. Researchers are encouraged to participate in both the COCO and Mapillary Detection Tasks (the tasks share identical data formats and evaluation metrics). Please also see the related COCO 2018 <a href="#keypoints-challenge2018">Keypoint</a>, <a href="#stuff-challenge2018">Stuff</a>, and <a href="#panoptic-challenge2018">Panoptic</a> tasks. Whereas the detection task addresses <i>thing</i> classes (person, car, elephant), the <a href="#stuff-challenge2018">stuff</a> task focuses on <i>stuff</i> classes (grass, wall, sky) and the newly introduced <a href="#panoptic-challenge2018">panoptic</a> task addresses both simultaneously.</p>
<p>The COCO train, validation, and test sets, containing more than 200,000 images and 80 object categories, are available on the <a href="#download">download</a> page. All object instances are annotated with a detailed segmentation mask. Annotations on the training and validation sets (with over 500,000 object instances segmented) are publicly available.</p>
<p>This is the fourth COCO detection challenge and it closely follows the <a href="#detections-challenge2017">COCO 2017 Detection Challenge</a>. In particular, the same data, metrics, and guidelines are being used for this year's challenge. The primary difference for 2018 is that <i>only the instance segmentation task will be featured at the challenge, with winners being invited to present at the workshop</i>. For detection with bounding boxes outputs, researchers may continue to submit to test-dev, but not to test-challenge, and results will not be presented at the workshop. As detection has steadily advanced, the purpose of this change is to encourage the community to focus on the more challenging and visually informative instance segmentation task.</p>
<p><b>Note: evaluation servers for the 2018 challenge will be open soon. For testing 2017 servers may be used for now.</b></p>

<h1>2. Dates</h1>
<div class="json">
  <div class="jsonktxt fontBlue">August 10, 2018</div><div class="jsonvtxt">Submission deadline (11:59 PST)</div>
  <div class="jsonktxt">August 26, 2018</div><div class="jsonvtxt">Challenge winners notified</div>
  <div class="jsonktxt">September 9, 2018</div><div class="jsonvtxt">Winners present at ECCV 2018 Workshop</div>
</div>

<h1>3. Organizers</h1>
<div>Tsung-Yi Lin (Google Brain)</div>
<div>Genevieve Patterson (MSR, Trash TV)</div>
<div>Matteo Ruggero Ronchi (Caltech)</div>
<div>Yin Cui (Cornell Tech)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Ross Girshick (Facebook AI Research)</div>
<div>Piotr Doll√°r (Facebook AI Research)</div>

<h1>4. Award Committee</h1>
<div>Genevieve Patterson (MSR, Trash TV)</div>
<div>Matteo Ruggero Ronchi (Caltech)</div>
<div>Yin Cui (Cornell Tech)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Serge Belongie (Cornell Tech)</div>
<div>Lubomir Bourdev (WaveOne, Inc.)</div>
<div>James Hays (Georgia Tech)</div>
<div>Pietro Perona (Caltech)</div>
<div>Deva Ramanan (CMU)</div>

<h1>5. Task Guidelines</h1>
<p>Participants are recommended but not restricted to train their algorithms on COCO 2017 train and val sets. The <a href="#download">download</a> page has links to all COCO 2017 images and ground-truth annotations as well as details of the annotation format. The COCO test set is divided into two splits: test-dev and test-challenge. Test-dev is as the default test set for testing under general circumstances and is used to maintain a public <a href="#detections-leaderboard">leaderboard</a> that is updated upon submission. Test-challenge is used for the workshop competition; results will be revealed during the workshop at ECCV 2018. When participating in this task, please specify any and all external data used for training in the "method description" when uploading results to the evaluation server. A more thorough explanation of all these details is available on the <a href="#guidelines">guidelines</a> page, please be sure to review it carefully prior to participating. Results in the correct <a href="#format-results">format</a> must be <a href="#upload">uploaded</a> to the <a href="https://competitions.codalab.org/competitions/5181" target="_blank">detection evaluation server</a>. The <a href="#detections-eval">detection evaluation</a> page lists detailed information regarding how results will be scored. Challenge participants with the most successful and innovative methods will be invited to present at the workshop.</p>

<h1>6. Tools and Instructions</h1>
<p>We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO API, please visit our <a href="https://github.com/cocodataset/cocoapi">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="#download">download</a> page. Due to the large size of COCO and the complexity of this task, the process of participating may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="#download">download</a>, <a href="#format-results">format</a>, <a href="#guidelines">guidelines</a>, <a href="#upload">upload</a>, and <a href="#detections-eval">evaluation</a> pages. For additional questions, please contact <a href="mailto:info@cocodataset.org">info@cocodataset.org</a>.</p>
