<p class="fontTitle">COCO 2018 Stuff Segmentation Task</p>
<p align="center"><img src="images/stuff-challenge-splash.png" class="wide"/></p>

<p class="fontTitle">1. Overview</p>
<p>The COCO 2018 Stuff Segmentation Task is designed to push the state of the art in semantic segmentation of <i>stuff</i> classes. Whereas the <a href="#detections-challenge2018">COCO 2018 Detection Task</a> addresses <i>thing</i> classes (person, car, elephant), this task focuses on <i>stuff</i> classes (grass, wall, sky). For full details of the stuff segmentation task please see the <a href="#stuff-eval">stuff evaluation</a> page. Note: the newly introduced <a href="#panoptic-challenge2018">Panoptic Segmentation</a> task addresses recognition of both things and stuff classes simultaneously.</p>
<p>Things are objects with a specific size and shape, that are often composed of parts. Stuff classes are background materials that are defined by homogeneous or repetitive patterns of fine-scale properties, but have no specific or distinctive spatial extent or shape. Why the focus on stuff? Stuff covers about 66% of the pixels in COCO. It allows us to explain important aspects of an image, including scene type; which thing classes are likely to be present and their location; as well as geometric properties of the scene. The COCO 2018 Stuff Segmentation Task builds on the COCO-Stuff project as described on <a href="https://github.com/nightrome/cocostuff">this website</a> and in this <a href="https://arxiv.org/abs/1612.03716">research paper</a>. This task includes and extends the original dataset release. Please note that in order to scale annotation, stuff segmentations were collected on superpixel segmentations of an image.</p>
<p>The stuff segmentation task will <i>not</i> be featured at the <a>Joint COCO and Mapillary Recognition Challenge Workshop</a> at ECCV 2018. <b>Researchers may continue to submit to test-dev, but not to test-challenge, and results will not be presented at the workshop.</b> Instead, we encourage researchers to participate in the upcoming <a href="#panoptic-challenge2018">Panoptic Segmentation</a> task which subsumes both stuff and instance segmentation and presents new challenges for the community.</p>
<p>The task includes 55K COCO images (train 40K, val 5K, test-dev 5K, test-challenge 5K) with annotations for 91 stuff classes and 1 'other' class. The stuff annotations cover 38M superpixels (10B pixels) with 296K stuff regions (5.4 stuff labels per image). Annotations for train and val are now available for <a href="#download">download</a>, while test set annotations will remain private. <b>Note: for 2018 the entire COCO dataset has been annotated with stuff segmentations. Updated data and info coming soon.</b></p>
<p><b>Note: evaluation servers for the 2018 annotations will be open soon. For testing 2017 servers may be used for now.</b></p>

<p class="fontTitle">2. Organizers</p>
<div>Holger Caesar (Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Tsung-Yi Lin (Google Brain)</div>
<div>Piotr Doll√°r (Facebook AI Research)</div>
<div>Vittorio Ferrari (Google, Edinburgh)</div>

<p class="fontTitle">3. Dates</p>
<div class="json">
  <div class="jsonvtxt"><b>No challenge in 2018, hence no deadlines. Test-dev remains open.</b></div>
</div>

<p class="fontTitle">4. Task Guidelines</p>
<p>Participants are recommended but not restricted to train their algorithms on COCO 2017 train and val sets. The <a href="#download">download</a> page has links to all COCO 2017 images and ground-truth annotations as well as details of the annotation format. The COCO test set is divided into two splits: test-dev and test-challenge. Test-dev is as the default test set for testing under general circumstances and is used to maintain a public <a href="#stuff-leaderboard">leaderboard</a> that is updated upon submission. Test-challenge is currently not being used for stuff segmentation. When participating in this task, please specify any and all external data used for training in the "method description" when uploading results to the evaluation server. A more thorough explanation of all these details is available on the <a href="#guidelines">guidelines</a> page, please be sure to review it carefully prior to participating. Results in the correct <a href="#format">format</a> must be <a href="#upload">uploaded</a> to the <a href="https://competitions.codalab.org/competitions/17443" target="_blank">stuff evaluation server</a>. The <a href="#stuff-eval">stuff evaluation</a> page lists detailed information regarding how results will be scored.</p>

<p class="fontTitle">5. Tools and Instructions</p>
<p>We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO Stuff API, please visit our <a href="https://github.com/nightrome/coco">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="#download">download</a> and <a href="#stuff-eval">stuff evaluation</a> pages. <b>Please note that this code is currently not part of the main COCO API repository.</b> Due to the large size of COCO and the complexity of this task, the process of participating may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="#download">download</a>, <a href="#format">format</a>, <a href="#guidelines">guidelines</a>, <a href="#upload">upload</a>, and <a href="#stuff-eval">evaluation</a> pages. For additional questions, please contact <a href="mailto:info@cocodataset.org">info@cocodataset.org</a>.</p>
