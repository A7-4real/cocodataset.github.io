<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">1. Captions Upload</p>
<p>This page describes the <i>upload instructions</i> for submitting results to the caption <a href="https://www.codalab.org/competitions/3221" target="_blank">evaluation server</a>. Before proceeding, please review the <a href="#format">results format</a> and <a href="#captions-eval">evaluation details</a>. Submitting results allows you to participate in the <a href="#captions-challenge2015">COCO Captioning Challenge 2015</a> and compare results to the state-of-the-art on the captioning <a href="#captions-leaderboard">leaderboard</a>.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">2. Competition Details</p>
<p><i>Training Data</i>: The recommended training set for the captioning challenge is the COCO 2014 Training Set. The COCO 2014 Validation Set may also be used for training when submitting results on the test set. External data of any form is allowed (except any form of annotation on the COCO Testing set is forbidden). Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server.</p>
<p>Please limit the number of entries to the captioning challenge to a reasonable number, e.g. one entry per paper. To avoid overfitting to the test data, the <i>number of submissions per user is limited to 1 upload per day and a maximum of 5 submissions per user</i>. It is not acceptable to create multiple accounts for a single project to circumvent this limit. The exception to this is if a group publishes two papers describing unrelated methods, in this case both sets of results can be submitted for evaluation.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">3. Enter The Competition</p>
<p>First you need to create an account on <a href="https://codalab.org" target="_blank">CodaLab</a>. From your account you will be able to participate in all COCO challenges.</p>
<p>Before uploading your results to the evaluation server, you will need to create two JSON files containing your captioning results in the correct <a href="#format">results format</a>. One file should correspond to your results on the 2014 validation dataset, and the other to the 2014 test dataset. Both sets of results are required for submission. Your files should be named as follows:</p>
<div class="json fontMono">
  <div class="jsonreg">
    <b>results.zip</b><br/>
    &nbsp; captions_val2014_[alg]_results.json<br/>
    &nbsp; captions_test2014_[alg]_results.json
  </div>
</div>
<p>Replace [alg] with your algorithm name and place both files into a single zip file named "results.zip".</p>
<p>To submit your zipped result file to the <a href="https://www.codalab.org/competitions/3221" target="_blank">COCO Captioning Challenge</a> click on the “Participate” tab on the CodaLab webpage. When you select “Submit / View Results” you will be given the option to submit new results. Please fill in the required fields and click “Submit”. A pop-up will prompt you to select the results zip file for upload. After the file is uploaded the evaluation server will begin processing. To view the status of your submission please select “Refresh Status”. Please be patient, the evaluation may take quite some time to complete. If the status of your submission is “Failed” please check to make sure your files are named correctly, they have the right <a href="#format">format</a>, and your zip file contains two files corresponding to the validation and testing datasets.</p>
<p>After you submit your results to the evaluation server, you can control whether your results are publicly posted to the CodaLab leaderboard. To toggle the public visibility of your results please select either “post to leaderboard” or “remove from leaderboard”. For now only one result can be published to the leaderboard at any time, we may change this in the future. After your results are posted to the CodaLab leaderboard, your captions on the validation dataset will be publicly available. Your captions on the test set will not be publicly released.</p>
<p>In addition to the <a href="https://www.codalab.org/competitions/3221#results" target="_blank">CodaLab leaderboard</a>, we also host our own more detailed <a href="#captions-leaderboard">leaderboard</a> that includes additional results and method information (such as paper references). Note that the CodaLab leaderboard may contain results not yet migrated to our own leaderboard.</p>

<!------------------------------------------------------------------------------------------------>
<p class="fontTitle">4. Download Evaluation Results</p>
<p>After evaluation is complete and the server shows a status of “Finished”, you will have the option to download your evaluation results by selecting “Download evaluation output from scoring step.” The zip file will contain five files:</p>
<div class="json fontMono">
  <div class="jsonreg">
    <div class="jsonblk">
      captions_val2014_[alg]_evalimgs.json<br/>
      captions_val2014_[alg]_eval.json<br/>
      captions_test2014_[alg]_eval.json<br/>
      metadata<br/>
      scores.txt
    </div>
    <div class="jsonblk">
      % per image evaluation on val <br/>
      % aggregated evaluation on val <br/>
      % aggregated evaluation on test <br/>
      % auto generated (safe to ignore) <br/>
      % auto generated (safe to ignore)
    </div>
  </div>
</div>
<p>The format of the evaluation files is described on the <a href="#captions-eval">caption evaluation</a> page. Please note that the *_evalImgs.json file is only available for download on the validation dataset, and not the test set.</p>
