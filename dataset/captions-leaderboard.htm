<script>
var metricsMain = [
  ['CIDEr-D', '<a href="http://arxiv.org/pdf/1411.5726.pdf" target="_blank"> CIDEr: Consensus-based Image Description Evaluation</a>'],
  ['METEOR',  '<a href="http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf" target="_blank">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</a>'],
  ['Rouge-L', '<a href="http://anthology.aclweb.org/W/W04/W04-1013.pdf" target="_blank"> ROUGE: A Package for Automatic Evaluation of Summaries</a>'],
  ['BLEU',    '<a href="http://www.aclweb.org/anthology/P02-1040.pdf" target="_blank"> BLEU: a Method for Automatic Evaluation of Machine Translation</a>'],
  ['SPICE',    '<a href="https://arxiv.org/pdf/1607.08822.pdf" target="_blank"> SPICE: Semantic Propositional Image Caption Evaluation</a>']
];
var metricsChallenge = [
  ['M1', 'Percentage of captions that are evaluated as better or equal to human caption.'],
  ['M2', 'Percentage of captions that pass the Turing Test.'],
  ['M3', 'Average correctness of the captions on a scale 1-5 (incorrect - correct).'],
  ['M4', 'Average amount of detail of the captions on a scale 1-5 (lack of details - very detailed).'],
  ['M5', 'Percentage of captions that are similar to human description.']
];
var ranksChallenge = [
  ['Google', 5, 4, 9, '1st(tie)'],
  ['MSR', 4, 5, 9, '1st(tie)'],
  ['MSR Captivator', 2, 3, 5, '3rd(tie)'],
  ['Montreal/Toronto', 3, 2, 5, '3rd(tie)'],
  ['Berkeley LRCN', 1, 1, 2, '5th']
];
// initialize and format DataTables https://www.datatables.net/
var propsRefs = { 'paging':false, 'info':false, 'filter':false, 'sort':false, 'autoWidth':false };
propsRefs.columnDefs = [{'targets':0,'sWidth':'25%'}];
var propsData = { 'paging':false, 'info':false, 'filter':false, 'order':[[1, 'desc']] };
propsData.columnDefs = [{'targets':['_all'],'orderSequence':['desc','asc']}];
$('#leaderboard_caption').find('#metrics').DataTable(propsRefs).rows.add(metricsMain).draw();
$('#leaderboard_caption_challenge').find('#metrics').DataTable(propsRefs).rows.add(metricsChallenge).draw();
$('#leaderboard_caption_challenge').find('#rank').DataTable(propsData).rows.add(ranksChallenge).draw();

var types = ["cap_c5", "cap_c40"];
var metrics = ["CIDEr", "METEOR", "ROUGE_L", "Bleu_1", "Bleu_2", "Bleu_3", "Bleu_4", "SPICE"];
var table = "leaderboard_caption";
var isSetTeam = false;
$( function() { initLeaderboard(types, metrics, table, isSetTeam);
                for( var i=0; i<types.length; i++ ) $('#a_'+types[i]).click( function(){$('#leaderboard_caption_challenge').hide(); $('#leaderboard_caption').show();})});

var types2 = ["cap_challenge2015"];
var metrics2 = ["q1", "q2", "q3", "q4", "q5"];
var table2 = "leaderboard_caption_challenge";
var isSetTeam2 = false;
$( function() { initLeaderboard(types2, metrics2, table2, isSetTeam2);
                for( var i=0; i<types2.length; i++ ) $('#a_'+types2[i]).click( function(){$('#leaderboard_caption_challenge').show(); $('#leaderboard_caption').hide();})});
$('#leaderboard_caption_challenge').hide();
</script>

<!------------------------------------------------------------------------------------------------>
<style>
  #leaderboard_caption td { padding: 6px 4px; vertical-align:middle }
  #leaderboard_caption th { padding: 6px 16px 6px 0px; text-align:right; width:36px }
  #leaderboard_caption_challenge td { padding: 6px 4px; vertical-align:middle }
  #leaderboard_caption_challenge th { padding: 6px 16px 6px 0px; text-align:right; width:36px }
</style>
<div style="margin:10px 0px; display:inline-block">
  <ul class="nav nav-pills">
    <li class="active"><a data-toggle="tab" style="padding:10px" id="a_cap_c5">Table-C5</a></li>
    <li><a data-toggle="tab" style="padding:10px" id="a_cap_c40">Table-C40</a></li>
    <li><a data-toggle="tab" style="padding:10px" id="a_cap_challenge2015">Challenge2015</a></li>
  </ul>
</div>
<div class="tab-content" id="leaderboard_caption" style="font-size:12px">
  <table id="data" class="table order-column hover">
    <thead>
      <tr style="font-size:12px;margin-left:-2px;">
        <th style="width:100px"></th>
        <th>CIDEr-D</th>
        <th>METEOR</th>
        <th>Rouge-L</th>
        <th>BLEU-1</th>
        <th>BLEU-2</th>
        <th>BLEU-3</th>
        <th>BLEU-4</th>
        <th>SPICE</th>
        <th>date</th>
      </tr>
    </thead>
  </table>
  <p class="fontTitle" style="margin-bottom:0px">Metrics</p>
    <table id="metrics" style="margin-bottom:10px" class="table table-striped hover">
      <thead><tr>
        <th></th>
        <th></th>
      </tr></thead>
    </table>
    <div>
      For the details of data collection and evaluation, please read <a href="http://arxiv.org/pdf/1504.00325.pdf" target="_blank">Microsoft COCO Captions: Data Collection and Evaluation Server</a>.
    </div>
  <p class="fontTitle" style="margin-bottom:0px">References</p>
  <table id="refs" class="table table-striped hover" style="font-size:13px">
    <thead><tr>
      <th></th>
      <th></th>
      <th></th>
    </tr></thead>
  </table>
</div>

<div class="tab-content" id="leaderboard_caption_challenge" style="font-size:12px;">
  <table id="data" class="table order-column hover" style=" width:70%;">
    <thead>
      <tr>
        <th style="width:100px"></th>
        <th>M1</th>
        <th>M2</th>
        <th>M3</th>
        <th>M4</th>
        <th>M5</th>
        <th>date</th>
      </tr>
    </thead>
  </table>
  <p class="fontTitle" style="margin-bottom:10px">Metrics</p>
  <div>
    We conducted a human study to understand how satisfactory are the results obtained from the captions submitted in the COCO captioning challenge. We were interested in three main points:
    <br/><br/>
    <ul>
      <li> Which algorithm produces the best captions?</li>
      <li> What are the factors determining which is the best algorithm?</li>
      <li> Do the algorithms produce captions resembling human-generated sentences?</li>
    </ul>
    To address the above questions we developed five Graphical User Interfaces (GUI) on the Amazon Mechanical Turk (AMT) platform to collect human responses. From the responses we designed the following five metrics
  </div>
  <table id="metrics" class="table table-striped hover">
    <thead><tr>
      <th></th>
      <th></th>
    </tr></thead>
  </table>
  <p class="fontTitle" style="margin-bottom:10px">Ranking</p>
  <div>
    The ranking for the competition was based on the results from M1 and M2. The other metrics have been used as diagnostic and interpretation of the results. Points are assigned to the top 5 teams for:
  </div>
  <table id="rank" style="width:60%" class="table table-striped hover">
    <thead><tr>
      <th></th>
      <th>M1</th>
      <th>M2</th>
      <th>TOTAL</th>
      <th>Ranking</th>
    </tr></thead>
  </table>
  <p class="fontTitle" style="margin-bottom:0px">References</p>
  <table id="refs" class="table table-striped hover" style="font-size:13px">
    <thead><tr>
      <th></th>
      <th></th>
      <th></th>
    </tr></thead>
  </table>
</div>