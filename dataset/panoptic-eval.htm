<h1>1. Panoptic Evaluation</h1>
<p>This page describes the <i>panoptic evaluation metrics</i> used by COCO. The evaluation code provided here can be used to obtain results on the publicly available COCO validation set. It computes multiple metrics described below. To obtain results on the COCO test set, for which ground-truth annotations are hidden, generated results must be <a href="#upload">uploaded</a> to the evaluation server. The exact same evaluation code, described below, is used to evaluate results on the test set.</p>
<p><b>Note: evaluation code will be available in June. The evaluation server will be live a bit later. We appreciate your patience as we complete development and testing.</b></p>

<h1>2. Panoptic Quality Overview</h2>
<p>Panoptic segmentation aims to unify the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). Existing metrics are specialized for either semantic or instance segmentation and cannot be used to evaluate the joint task involving both stuff and thing classes. Rather than using a heuristic combination of disjoint metrics for the two tasks, the panoptic task introduces a new Panoptic Quality (PQ) metric. PQ evaluates performance for all categories, including both stuff and thing categories, in a unified manner.</p>
<p>Computing PQ involves two steps: (1) segment matching and (2) PQ computation given the matches. Both steps are simple and efficient. First, for each image, the ground truth and predicted segments are matched with an IoU threshold 0.5. Since panoptic segmentation requires non-overlapping segments, if a threshold of 0.5 is used, <i>the matching is unique</i> (a simple proof is in the paper). That is, there can be at most one match per segment and so it is trivial to obtain the matches. After matching, each segment falls into one of three sets: TP (matched pairs), FP (unmatched predicted segments), and FN (unmatched ground truth segments). Given the TP, FP, and FN, the Panoptic Quality (PQ) metric for each category is simply:</p>
<div class="json fontMono">
  $$ \text{PQ} = \frac{\sum_{(p, g) \in \mathit{TP}} \text{IoU}(p, g)}{|\mathit{TP}| + \frac{1}{2}|\mathit{FP}| + \frac{1}{2}|\mathit{FN}|} $$
</div>
<p>PQ is computed per-category and results are averaged across categories. Predicted segments that have significant overlaps with unlabeled or crowd regions are filtered out from the FP set. More details of the metric can be found in the paper. Finally, PQ can be decomposed as the multiplication of a segmentation quality (SQ) term and a recognition quality (RQ) term. The decomposition of PQ = SQ * RQ is useful for providing additional insights for analysis:</p>
<div class="json fontMono">
  $$ {\text{PQ}} = \underbrace{\frac{\sum_{(p, g) \in TP} \text{IoU}(p, g)}{\vphantom{\frac{1}{2}}|TP|}}_{\text{segmentation quality (SQ)}} \times \underbrace{\frac{|TP|}{|TP| + \frac{1}{2} |FP| + \frac{1}{2} |FN|}}_{\text{recognition quality (RQ)}} $$
</div>
<p>For more details about the panoptic task and metrics please see the <a href="https://arxiv.org/abs/1801.00868">panoptic segmentation paper</a>.</p>

<h1>3. Metrics</h1>
<p>The following 9 metrics are used for characterizing the performance of a panoptic segmenter on COCO:</p>
<div class="json fontMono">
  <div class="jsonreg"><b>Average Panoptic Metrics:</b></div>
  <div class="jsonk">PQ</div><div class="jsonv">% Panoptic Quality <b>(primary challenge metric)</b></div>
  <div class="jsonk">SQ</div><div class="jsonv">% Segmentation Quality component of PQ</div>
  <div class="jsonk">RQ</div><div class="jsonv">% Recognition Quality component of PQ</div>
  <div class="jsonreg"><b>Panoptic Metrics for Things Categories:</b></div>
  <div class="jsonk">PQ<sup>Th</sup></div><div class="jsonv">% PQ for things categories only</div>
  <div class="jsonk">SQ<sup>Th</sup></div><div class="jsonv">% SQ for things categories only</div>
  <div class="jsonk">RQ<sup>Th</sup></div><div class="jsonv">% RQ for over things categories only</div>
  <div class="jsonreg"><b>Panoptic Metrics for Stuff Categories:</b></div>
  <div class="jsonk">PQ<sup>St</sup></div><div class="jsonv">% PQ for stuff categories only</div>
  <div class="jsonk">SQ<sup>St</sup></div><div class="jsonv">% SQ for stuff categories only</div>
  <div class="jsonk">RQ<sup>St</sup></div><div class="jsonv">% RQ for stuff categories only</div>
</div></br>
<ol>
  <li>PQ will determine the challenge winner. Remaining metrics are reported for analysis purposes.</li>
  <li>PQ (and likewise SQ and RQ) are averaged over all categories, including both stuff and thing categories.</li>
  <li>Please note that per-category, PQ = SQ * RQ (but averages across categories will not decompose this way).</li>
</ol>

<h1>4. Evaluation Code</h1>
<h2 class="fontBold">Coming soon!</h2>

<script type="text/javascript">
// forced refresh of MathJax to hopefully force equations to render properly
if(window.MathJax) { MathJax.Hub.Queue(["Typeset",MathJax.Hub]); }
</script>
