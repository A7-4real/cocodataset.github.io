<p class="fontTitle">COCO 2017 Stuff Segmentation Challenge</p>
<p class="fontBig">The <a href="https://places-coco2017.github.io/#winners">Challenge Winners</a> have now been announced! Up-to-date results are on the <a href="#stuff-leaderboard">stuff leaderboard</a>. Note that the evaluation server on test-dev remains open for uploading of new results.</p>
<p align="center"><img src="images/stuff-challenge-splash.png" class="wide"/></p>

<p class="fontTitle">1. Overview</p>
<p>The COCO 2017 Stuff Segmentation Challenge is designed to push the state of the art in semantic segmentation of <i>stuff</i> classes. Whereas the <a href="#detections-challenge2017">COCO 2017 Detection Challenge</a> addresses <i>thing</i> classes (person, car, elephant), this challenge focuses on <i>stuff</i> classes (grass, wall, sky). For full details of this task please see the <a href="#stuff-eval">stuff evaluation</a> page.</p></p>
<p>Things are objects with a specific size and shape, that are often composed of parts. Stuff classes are background materials that are defined by homogeneous or repetitive patterns of fine-scale properties, but have no specific or distinctive spatial extent or shape. Why the focus on stuff? Stuff covers about 66% of the pixels in COCO. It allows us to explain important aspects of an image, including scene type; which thing classes are likely to be present and their location; as well as geometric properties of the scene. The COCO 2017 Stuff Segmentation Challenge builds on the COCO-Stuff project as described on <a href="https://github.com/nightrome/cocostuff">this website</a> and in this <a href="https://arxiv.org/abs/1612.03716">research paper</a>. This challenge includes and extends the original dataset release.  Please note that in order to scale annotation, stuff segmentations were collected on superpixel segmentations of an image.</p>
<p>This challenge is part of the <a href="https://places-coco2017.github.io/">Joint COCO and Places Recognition Challenge Workshop</a> at ICCV 2017. For further details about the joint workshop please visit the workshop website. Please also see the concurrent COCO 2017 <a href="#detections-challenge2017">Detection</a> and <a href="#keypoints-challenge2017">Keypoint</a> Challenges.</p>
<p>The challenge includes 55K COCO images (train 40K, val 5K, test-dev 5K, test-challenge 5K) with annotations for 91 stuff classes and 1 'other' class. The stuff annotations cover 38M superpixels (10B pixels) with 296K stuff regions (5.4 stuff labels per image). <b>Annotations for train and val are now available for <a href="#download">download</a></b>, while test set annotations will remain private.</p>

<p class="fontTitle">2. Dates</p>
<div class="json">
  <div class="jsonktxt fontBlue">October 8, 2017</div><div class="jsonvtxt">[Extended] Submission deadline (11:59 PST)</div>
  <div class="jsonktxt">October 15, 2017</div><div class="jsonvtxt">Challenge winners notified</div>
  <div class="jsonktxt">October 29, 2017</div><div class="jsonvtxt">Winners present at ICCV 2017 Workshop</div>
</div>

<p class="fontTitle">3. Organizers</p>
<div>Holger Caesar (Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Tsung-Yi Lin (Cornell Tech)</div>
<div>Piotr Doll√°r (Facebook AI Research)</div>
<div>Vittorio Ferrari (Google, Edinburgh)</div>

<p class="fontTitle">4. Award Committee</p>
<div>Holger Caesar (Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Tsung-Yi Lin (Cornell Tech)</div>
<div>Genevieve Patterson (Microsoft Research)</div>
<div>Matteo Ruggero Ronchi (Caltech)</div>
<div>Yin Cui (Cornell Tech)</div>
<div>Serge Belongie (Cornell Tech)</div>
<div>Lubomir Bourdev (WaveOne, Inc.)</div>
<div>James Hays (Georgia Tech)</div>
<div>Pietro Perona (Caltech)</div>
<div>Deva Ramanan (CMU)</div>
<div>Vittorio Ferrari (Google, Edinburgh)</div>

<p class="fontTitle">5. Challenge Guidelines</p>
<p>Competitors are recommended but not restricted to train their algorithms on COCO 2017 train and val sets. The <a href="#download">download</a> page has links to all COCO 2017 images and ground-truth annotations as well as details of the annotation format. The COCO test set is divided into two splits: test-dev and test-challenge. Test-dev is as the default test set for testing under general circumstances and is used to maintain a public <a href="#stuff-leaderboard">leaderboard</a> that is updated upon submission. Test-challenge is used for the workshop competition; results will be revealed during the workshop at ICCV 2017. Please note that while only a portion of test-dev and test-challenge are annotated with stuff labels at this time, <b>submissions must provide results on all test images.</b> When participating in the challenge, please specify any and all external data used for training in the "method description" when uploading results to the evaluation server. A more thorough explanation of all these details is available on the <a href="#guidelines">guidelines</a> page, please be sure to review it carefully prior to participating. By the challenge deadline, results in the correct <a href="#format">format</a> must be <a href="#upload">uploaded</a> to the <a href="https://competitions.codalab.org/competitions/17443" target="_blank">stuff evaluation server</a>. The <a href="#stuff-eval">stuff evaluation</a> page lists detailed information regarding how submissions will be scored. Challenge participants with the most successful and innovative methods will be invited to present at the workshop.</p>

<p class="fontTitle">6. Tools and Instructions</p>
<p>We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO Stuff API, please visit our <a href="https://github.com/nightrome/coco">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="#download">download</a> and <a href="#stuff-eval">stuff evaluation</a> pages. <b>Please note that this code is currently not part of the main COCO API repository.</b> Due to the large size of COCO and the complexity of this challenge, the process of competing in this challenge may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="#download">download</a>, <a href="#format">format</a>, <a href="#guidelines">guidelines</a>, <a href="#upload">upload</a>, and <a href="#stuff-eval">evaluation</a> pages. For additional questions, please contact <a href="mailto:info@cocodataset.org">info@cocodataset.org</a>.</p>
