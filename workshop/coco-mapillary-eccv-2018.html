<!DOCTYPE html>
<head>
  <meta charset="UTF-8">
  <title>COCO + Mapillary 2018 | ECCV 2018</title>
  <link rel="stylesheet" href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/themes/smoothness/jquery-ui.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" />
  <link rel="stylesheet" href="../other/cocostyles.css" />
</head>
<body>
  <div class="eccv18header">
    <div class="eccv18title">COCO + Mapillary</div>
    <div class="eccv18subtitle">Joint Recognition Challenge Workshop at ECCV 2018</div>
  </div>

  <div id="content">
    <h1>Table of Contents</h1>
    <p class="fontBigger">
      1. <a href="#overview">Overview</a><br>
      2. <a href="#dates">Dates</a><br>
      3. <a href="#coco-challenges">COCO Challenges</a> [<a href="../index.htm#detection-2018">detection</a>, <a href="../index.htm#panoptic-2018">panoptic</a>, <a href="../index.htm#keypoints-2018">keypoints</a>]<br>
      4. <a href="#mapillary-challenges">Mapillary Challenges</a> [<a href="http://research.mapillary.com/eccv18#detection">detection</a>, <a href="http://research.mapillary.com/eccv18#panoptic">panoptic</a>]<br>
      5. <a href="#schedule">Workshop Schedule</a><br>
      6. <a href="#speakers">Invited Speakers</a><br>
    </p>

    <a name="overview"></a>
    <h1>1. Overview</h1>
    <p>The goal of the joint COCO and Mapillary Workshop is to study object recognition in the context of scene understanding. While both the COCO and Mapillary challenges look at the general problem of visual recognition, the underlying datasets and the specific tasks in the challenges probe different aspects of the problem.</p>

    <p><a href="../index.htm">COCO</a> is a widely used visual recognition dataset, designed to spur object detection research with a focus on full scene understanding. In particular: detecting non-iconic views of objects, localizing objects in images with pixel level precision, and detection in complex scenes. <a href="https://vistas.mapillary.com/">Mapillary Vistas</a> is a new street-level image dataset with emphasis on high-level, semantic image understanding, with applications for autonomous vehicles and robot navigation. The dataset features locations from all around the world and is diverse in terms of weather and illumination conditions, capturing sensor characteristics, etc.</p>

    <p><a href="https://vistas.mapillary.com/">Mapillary Vistas</a> is complementary to COCO in terms of dataset focus and can be readily used for studying various recognition tasks in a visually distinct domain from COCO. COCO focuses on recognition in natural scenes, while Mapillary focuses on recognition of street-view scenes. <i>We encourage teams to participate in challenges across both datasets</i> to better understand the current landscape of datasets and methods.</p>

    <p>Challenge tasks: COCO helped popularize <a href="../index.htm#detection-2018">instance segmentation</a> and this year both COCO and Mapillary feature this task, where the goal is to simultaneously detect and segment each object instance. As detection has matured over the years, <b>COCO is no longer featuring the bounding-box detection task</b>. While the leaderboard will remain open, the bounding-box detection task is not a workshop challenge; instead we encourage researchers to focus on the more challenging and visually informative instance segmentation task. As in previous years, COCO features the popular person <a href="../index.htm#keypoints-2018">keypoint challenge</a> track.</p>

    <p>This year we are pleased to introduce the <a href="../index.htm#panoptic-2018">panoptic segmentation</a> task with the goal of advancing the state of the art in scene segmentation. Panoptic segmentation addresses both stuff and thing classes, unifying the typically distinct semantic and instance segmentation tasks. The definition of panoptic is “including everything visible in one view”, in this context panoptic refers to a unified, global view of segmentation. The aim is to generate coherent scene segmentations that are rich and complete, an important step toward real-world vision systems. For more details about the panoptic task, including evaluation metrics, please see this <a href="https://arxiv.org/abs/1801.00868">paper</a>. Both COCO and Mapillary will feature panoptic segmentation challenges.</p>

    <p>This workshop offers the opportunity to benchmark computer vision algorithms on the COCO and Mapillary Vistas datasets. The instance and panoptic segmentation tasks on the two datasets are the same, and we use unified data formats and evaluation criteria for both. We hope that jointly studying the unified tasks across two distinct visual domains will provide a highly comprehensive evaluation suite for modern visual recognition and segmentation algorithms and yield new insights.</p>

    <a name="dates"></a>
    <h1>2. Challenge Dates</h1>
    <div class="json">
      <div class="jsonktxt fontBlue">August 10, 2018</div><div class="jsonvtxt">Submission deadline (11:59 PST)</div>
      <div class="jsonktxt">August 26, 2018</div><div class="jsonvtxt">Challenge winners notified</div>
      <div class="jsonktxt">September 9, 2018</div><div class="jsonvtxt">Winners present at ECCV 2018 Workshop</div>
    </div>

    <a name="coco-challenges"></a>
    <h1>3. COCO Challenges</h1>
    <p><a href="http://cocodataset.org/">COCO</a> is an image dataset designed to spur object detection research with a focus on detecting objects in context. The annotations include instance segmentations for object belonging to 80 categories, stuff segmentations for 91 categories, keypoint annotations for person instances, and five image captions per image. The specific tracks in the COCO 2018 Challenges are (1) object detection with segmentation masks (instance segmentation), (2) person keypoint estimation, and (3) panoptic segmentation. We describe each next. Note: <i>neither the object detection task with bounding-box output nor the stuff segmentation task will be featured at the COCO 2018 challenge</i> (but evaluation servers remain open for both tasks).</p>

    <h2>3.1. COCO Object Detection Task</h2>
    <p><a href="../index.htm#detection-2018"><img src="../images/detection-splash.png" class="wide" /></a></p>
    <p>The COCO Object Detection Task is designed to push the state of the art in object detection forward. Note: only the detection task with object segmentation output (that is, instance segmentation) will be featured at the COCO 2018 challenge. For full details of this task please see the <a href="../index.htm#detection-2018">COCO Object Detection Task</a>.</p>

    <h2>3.2. COCO Panoptic Segmentation Task</h2>
    <p><a href="../index.htm#panoptic-2018"><img src="../images/panoptic-splash.png" class="wide" /></a></p>
    <p>The COCO Panoptic Segmentation Task has the goal of advancing the state of the art in scene segmentation. Panoptic segmentation addresses both stuff and thing classes, unifying the typically distinct semantic and instance segmentation tasks. For full details of this task please see the <a href="../index.htm#panoptic-2018">COCO Panoptic Segmentation Task</a>.</p>

    <h2>3.3. COCO Keypoint Detection Task</h2>
    <p><a href="../index.htm#keypoints-2018"><img src="../images/keypoints-splash.png" class="wide" /></a></p>
    <p>The COCO Keypoint Detection Task requires localization of person keypoints in challenging, uncontrolled conditions. The keypoint task involves simultaneously detecting people <i>and</i> localizing their keypoints (person locations are <i>not</i> given at test time). For full details of this task please see the <a href="../index.htm#keypoints-2018">COCO Keypoint Detection Task</a>.</p>

    <a name="mapillary-challenges"></a>
    <h1>4. Mapillary Challenges</h1>
    <p>This year, <a href="http://research.mapillary.com/eccv18">Mapillary Research</a> is joining the popular COCO recognition tasks with the <a href="https://vistas.mapillary.com/"> Mapillary Vistas</a> dataset. Vistas is a diverse, pixel-accurate street-level image dataset for empowering autonomous mobility and transport at global scale. It has been designed and collected to cover diversity in appearance, richness of annotation detail, and geographic extent. The Mapillary challenges are based on the publicly available Vistas Research dataset, featuring:
    <ul>
      <li>28 stuff classes, 37 thing classes (w instance-specific annotations), and 1 void class</li>
      <li>25K high-resolution images (18K train, 2K val, 5K test; w average resolution of ~9 megapixels)</li>
      <li>Global geographic coverage including North and South America, Europe, Africa, Asia, and Oceania</li>
      <li>Highly variable weather conditions (sun, rain, snow, fog, haze) and capture times (dawn, daylight, dusk, night)</li>
      <li>Broad range of camera sensors, varying focal length, image aspect ratios, and different types of camera noise</li>
      <li>Different capturing viewpoints (road, sidewalks, off-road)</li>
    </ul>
    Challenge tracks based on the Mapillary Vistas dataset will be (1) object detection with segmentation masks (instance segmentation) and (2) panoptic segmentation, in line with COCO's detection and panoptic segmentation tasks, respectively.</p>

    <h2>4.1. Mapillary Vistas Object Detection Task</h2>
    <p><a href="http://research.mapillary.com/eccv18#detection"><img src="../images/mapillary-instance.jpg" class="wide" /></a></p>
    <p>The Mapillary Vistas Object Detection Task emphasizes recognizing individual instances of both static street-image objects (like street lights, signs, poles) but also dynamic street participants (like cars, pedestrians, cyclists). This task aims to push the state-of-the-art in instance segmentation, targeting critical perception tasks for autonomously acting agents like cars or transportation robots. For full details of this task please see the <a href="http://research.mapillary.com/eccv18#detection">Mapillary Vistas Object Detection Task</a>.</p>

    <h2>4.2. Mapillary Vistas Panoptic Segmentation Task</h2>
    <p><a href="http://research.mapillary.com/eccv18#panoptic"><img src="../images/mapillary-panoptic.png" class="wide" /></a></p>
    <p>The Mapillary Vistas Panoptic Segmentation Task targets the full perception stack for scene segmentation in street-images. Panoptic segmentation addresses both stuff and thing classes, unifying the typically distinct semantic and instance segmentation tasks. For full details of this task please see the <a href="http://research.mapillary.com/eccv18#panoptic">Mapillary Vistas Panoptic Segmentation Task</a>.</p>

    <a name="schedule"></a>
    <h1>5. Workshop Schedule - 09.09.2018</h1>
    Coming not so soon.

    <a name="speakers"></a>
    <h1>6. Invited Speakers </h1>
    <div>
      <a name="andreas"></a>
      <div class="speakerimg">
        <img class="wide img-rounded" src="../images/speakers/AndreasGeiger.jpg">
      </div>
      <div class="speakerbio">
        <h3><a href="http://www.cvlibs.net/" target="_blank">Andreas Geiger</a></h3>
        <p>MPI-IS and University of Tübingen</p>
        <p><i>Andreas Geiger is a full professor at the University of Tübingen and a group leader at the Max Planck Institute for Intelligent Systems. Prior to this, he was a visiting professor at ETH Zürich and a research scientist in the Perceiving Systems department of Dr. Michael Black at the MPI-IS. He received his PhD degree in 2013 from the Karlsruhe Institute of Technology. His research interests are at the intersection of 3D reconstruction, motion estimation and visual scene understanding. His work has been recognized with several prizes, including the 3DV best paper award, the CVPR best paper runner up award, the Heinz Maier Leibnitz Prize and the German Pattern Recognition Award. He serves as an area chair and associate editor for several computer vision conferences and journals (CVPR, ICCV, ECCV, PAMI, IJCV).</i></p>
      </div>
    </div>
    Additional speakers tba.
  </div>

  <div id="footer"><div>
    <a href="https://github.com/cocodataset/cocodataset.github.io" target="_blank">Github Page Source</a>
    <a href="../index.htm#termsofuse">Terms of Use</a>
  </div></div>
</body>
