<!DOCTYPE html>
<head>
  <meta charset="UTF-8">
  <title>COCO + LVIS | ECCV 2020</title>
  <link rel="stylesheet" href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/themes/smoothness/jquery-ui.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" />
  <link rel="stylesheet" href="../other/cocostyles.css" />
</head>
<body>
  <div class="eccv20header">
    <div class="eccv20title">COCO + LVIS</div>
    <div class="eccv20subtitle">Joint Recognition Challenge Workshop at ECCV 2020</div>
  </div>

  <div id="content">
    <h1>Table of Contents</h1>
    <ol class="fontBigger">
      <li><a href="#schedule">Workshop Schedule</a></li>
      <li><a href="#overview">Overview</a></li>
      <li><a href="#dates">Dates</a></li>
      <li><a href="#organizers">Organizers</a></li>
      <ul>
        <li><a href="#coco-organizers">COCO Organizers</a></li>
        <li><a href="#lvis-organizers">LVIS Organizers</a></li>
        <li><a href="#award-committee">Award Committee</a></li>
      </ul>
      <li><a href="#rules">Rules and Awards</a></li>
      <li><a href="#challenges">Challenge tracks</a></li>
    </ol>

    <a name="schedule"></a>
    <h1>1. Workshop Schedule - August 8, 2020</h1>
    <p><strong>Note: the time zone is UTC+1.<br>We will host two live sessions with panel discussion during the second one.</strong></p>
    <table class="table">
      <tr class="schedule-track">
        <th width=20%>0:00am and 4:00pm</th>
        <td width=60%>COCO introduction and results</td>
        <th width=20%></th>
      </tr>
      <tr class="schedule-competitor">
        <th width=20%>0:10am and 4:10pm</th>
        <td width=60%>COCO invited talks</td>
        <th width=20%></th>
      </tr>
      <tr class="schedule-track">
        <th width=20%>0:25am and 4:25pm</th>
        <td width=60%>LVIS introduction and results</td>
        <th width=20%>Agrim Gupta</th>
      </tr>
      <tr class="schedule-competitor">
        <th width=20%>0:35am and 4:35pm</th>
        <td width=60%>LVIS invited talks</td>
        <th width=20%></th>
      </tr>
      <tr class="schedule-break">
        <th width=20%>5:30pm</th>
        <td width=60%>Panel: The future of computer vision datasets, benchmarks, and challenges</td>
        <th width=20%>Award committee</th>
      </tr>
    </table>

    <a name="overview"></a>
    <h1>2. Overview</h1>
    <p>The goal of the joint COCO and LVIS Workshop is to study object recognition in the context of scene understanding. This workshop will host the COCO suite of challenges and a new challenge on large vocabulary instance segmentation (LVIS). While both the COCO and LVIS challenges look at the general problem of visual recognition, the specific tasks in the challenges probe different aspects of the problem.</p>

    <p><a href="../index.htm">COCO</a> is a widely used visual recognition dataset, designed to spur object detection research with a focus on full scene understanding. In particular: detecting non-iconic views of objects, localizing objects in images with pixel level precision, and detection of objects in complex scenes. The COCO dataset includes 330K images of complex scenes exhaustively annotated with 80 object categories with segmentation masks, 91 stuff categories with segmentation masks, person keypoint annotations, and 5 captions per image.</p>

    <p><a href="https://www.lvisdataset.org/">Large Vocabulary Instance Segmentation (LVIS)</a> includes high-quality instance segmentations for more than 1000 entry-level object categories. The LVIS dataset contains a long-tail of categories with few examples, making it a distinct challenge from COCO and exposes shortcomings and new opportunities in machine learning. We expect this dataset to inspire new methods in the detection research community. This year we plan to host the first challenge for LVIS, a new large vocabulary dataset.</p>

    <a name="dates"></a>
    <h1>3. Challenge Dates</h1>
    <div class="json">
      <div class="jsonktxt fontBlue">August 7, 2020</div><div class="jsonvtxt">Submission deadline (11:59 PM PST) <strong>(1 week extension)</strong> </div>
      <div class="jsonktxt">August 10, 2020</div><div class="jsonvtxt">Technical report submission deadline (11:59 PM PST)</div>
      <div class="jsonktxt">August 17, 2020</div><div class="jsonvtxt">Challenge winners notified</div>
      <div class="jsonktxt">August 21, 2020</div><div class="jsonvtxt">Presenter's slides and videos due (submitted to organizers)</div>
      <div class="jsonktxt">August 23, 2020</div><div class="jsonvtxt">ECCV 2020 Workshop</div>
    </div>

    <a name="organizers"></a>
    <h1>4. Organizers</h1>
    <a name="coco-organizers"></a>
    <h2>4.1. COCO</h2>
    <ul>
      <li><a href="https://alexander-kirillov.github.io/">Alexander Kirillov</a> (FAIR)</li>
      <li><a href="https://vision.cornell.edu/se3/people/tsung-yi-lin/">Tsung-Yi Lin</a> (Google Research)</li>
      <li><a href="https://ycui.me/">Yin Cui</a> (Google Research)</li>
      <li><a href="http://www.vision.caltech.edu/~mronchi/">Matteo Ruggero Ronchi</a> (California Institute of Technology)</li>
      <li><a href="https://nneverova.github.io/">Natalia Neverova</a> (FAIR)</li>
      <li><a href="mailto:vkhalidov@fb.com">Vasil Khalidov</a> (FAIR)</li>
      <li><a href="https://www.rossgirshick.info/">Ross Girshick</a> (FAIR)</li>
      <li><a href="https://pdollar.github.io/">Piotr Dollar</a> (FAIR)</li>
    </ul>
    <a name="lvis-organizers"></a>
    <h2>4.2. LVIS</h2>
    <ul>
      <li><a href="http://web.stanford.edu/~agrim/">Agrim Gupta</a> (Stanford)</li>
      <li><a href="https://www.rossgirshick.info/">Ross Girshick</a> (FAIR)</li>
    </ul>
    <a name="award-committee"></a>
    <h2>4.3. Award committee</h2>
    <ul>
      <li>Yin Cui (Google Research)</li>
      <li>Tsung-Yi Lin (Google Research)</li>
      <li>Alexander Kirillov (Facebook AI Research)</li>
      <li>Natalia Neverova (Facebook AI Research)</li>
      <li>Matteo Ruggero Ronchi (Caltech)</li>
      <li>Michael Maire (University of Chicago)</li>
      <li>Lubomir Bourdev (WaveOne, Inc.)</li>
      <li>James Hays (Georgia Tech)</li>
      <li>Larry Zitnick (Facebook AI Research)</li>
      <li>Ross Girshick (Facebook AI Research)</li>
      <li>Piotr Doll√°r (Facebook AI Research)</li>
    </ul>

    <a name="rules"></a>
    <h1>5. Rules and Awards</h1>
    <ul>
      <li>Participants must submit a <strong>technical report</strong> that includes a detailed ablation study of their submission via <a href="https://cmt3.research.microsoft.com/COCOLVIS2020">CMT</a>. For the technical report, use the following ECCV-based <a href="../files/tech_report_template_eccv20.zip">template</a>. Suggested length of the report is 2-7 pages. The reports will be made public. This report will substitute the short text description that we requested previously. Only submissions with the report will be considered for any award and will be put in the COCO leaderboard.</li>
      <li>This year for each challenge track we will have two different awards: <strong>best result award</strong> and <strong>most innovative award</strong>. The most innovative award will be based on the method description in the submitted technical reports and decided by the COCO award committee. The commitee will invite teams to present at the workshop based on the innovations of the submissions rather than the best scores.</li>
      <li>This year we will award single <strong>best paper award</strong> for the most innovative and successful solution across all challenges. The winner will be determined by the workshop organization committee.</li>
    </ul>

    <a name="coco-challenges"></a>
    <h1>6. COCO Challenges</h1>
    <p><a href="http://cocodataset.org/">COCO</a> is an image dataset designed to spur object detection research with a focus on detecting objects in context. The annotations include instance segmentations for object belonging to 80 categories, stuff segmentations for 91 categories, keypoint annotations for person instances, and five image captions per image. The specific tracks in the COCO 2018 Challenges are (1) object detection with segmentation masks (instance segmentation), (2) panoptic segmentation, (3) person keypoint estimation, and (4) DensePose. We describe each next. Note: <i>neither object detection with bounding-box outputs nor stuff segmentation will be featured at the COCO 2020 challenge</i> (but evaluation servers for both tasks remain open).</p>

    <a name="coco-detection"></a>
    <h2>6.1. COCO Object Detection Task</h2>
    <p><a href="../index.htm#detection-2020"><img src="../images/detection-splash.png" class="wide" /></a></p>
    <p>The COCO Object Detection Task is designed to push the state of the art in object detection forward. Note: only the detection task with object segmentation output (that is, instance segmentation) will be featured at the COCO 2019 challenge. For full details of this task please see the <a href="../index.htm#detection-2020">COCO Object Detection Task</a>.</p>

    <a name="coco-panoptic"></a>
    <h2>6.2. COCO Panoptic Segmentation Task</h2>
    <p><a href="../index.htm#panoptic-2020"><img src="../images/panoptic-splash.png" class="wide" /></a></p>
    <p>The COCO Panoptic Segmentation Task has the goal of advancing the state of the art in scene segmentation. Panoptic segmentation addresses both stuff and thing classes, unifying the typically distinct semantic and instance segmentation tasks. For full details of this task please see the <a href="../index.htm#panoptic-2020">COCO Panoptic Segmentation Task</a>.</p>

    <a name="coco-keypoints"></a>
    <h2>6.3. COCO Keypoint Detection Task</h2>
    <p><a href="../index.htm#keypoints-2020"><img src="../images/keypoints-splash.png" class="wide" /></a></p>
    <p>The COCO Keypoint Detection Task requires localization of person keypoints in challenging, uncontrolled conditions. The keypoint task involves simultaneously detecting people <i>and</i> localizing their keypoints (person locations are <i>not</i> given at test time). For full details of this task please see the <a href="../index.htm#keypoints-2020">COCO Keypoint Detection Task</a>.</p>

    <a name="coco-densepose"></a>
    <h2>6.4. COCO DensePose Task</h2>
    <p><a href="../index.htm#densepose-2020"><img src="../images/densepose-splash.png" class="wide" /></a></p>
    <p>The COCO DensePose Task requires dense estimation of human pose in challenging, uncontrolled conditions. The DensePose task involves simultaneously detecting people, segmenting their bodies and mapping all image pixels that belong to a human body to the 3D surface of the body. For full details of this task please see the <a href="../index.htm#densepose-2020">COCO DensePose Task</a>.</p>

    <a name="lvis-challenge"></a>
    <h1>7. LVIS Challenge</h1>
    <p><a href="http://www.lvisdataset.org/"><img src="../images/lvis-splash.png" class="wide" /></a></p>
    <p>LVIS is a new, large-scale instance segmentation dataset that features > 1000 object categories, many of which have very few training examples. LVIS presents a novel low-shot object detection challenge to encourage new research in object detection. For more information, please see <a href="http://www.lvisdataset.org/" target="_blank">LVIS challenge</a> page. </p>

  </div>

  <div id="footer"><div>
    <a href="https://github.com/cocodataset/cocodataset.github.io" target="_blank">Github Page Source</a>
    <a href="../index.htm#termsofuse">Terms of Use</a>
  </div></div>
</body>
